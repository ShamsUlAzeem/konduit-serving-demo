{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konduit Serving BMI Calculator Setup\n",
    "Body mass index (BMI) is a measure of body fat based on height and weight. The standard way of calculating BMI is through the formula below:\n",
    "\n",
    "$$ \\text{BMI}=\\frac{m} {{h}^2} $$\n",
    "\n",
    "$$ \\text{BMI} = \\text{Body Mass Index} $$\n",
    "\n",
    "$$ \\text{m} = \\text{mass (in kilograms/pounds)} $$\n",
    "\n",
    "$$ \\text{h} = \\text{height (in meters/feet)} $$\n",
    "\n",
    "### A new approach for estimating Body Mass Index using facial features with Konduit-Serving\n",
    "Measuring weight and height of individual people is time-consuming while also being error prone. In this notebook, we'll see how we can measure BMI of an individual just by looking at their facial features. The backend server used for taking image data and providing BMI values is served using konduit-serving, which is a high performance model server. Model training, gathering and preparing dataset is out of the scope of this notebook. The main workflow we'll look at is how to serve a model that can look at a person's face and answer back with a BMI value through REST API. We'll also be setting up a web server through konduit-serving \"custom endpoints\" that will make use of a webcam and label the canvas with the detected face along with the corresponding estimated BMI value. \n",
    "\n",
    "So, let's get started!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konduit Serving Components\n",
    "\n",
    "Before diving deep into the coding part, let's look at some of the few konduit-serving components in detail. \n",
    "\n",
    "### 1. CLI Interface\n",
    "Konduit-Serving comes with a CLI interface (with a `konduit` alias) that's responsible of taking care of most aspects of the application. The help command will describe most of what we are able to do with konduit-serving. Executing `konduit --help` command will show us the following output:\n",
    "\n",
    "```bash\n",
    "$ konduit --help\n",
    "-------------------------------------------------------------------------------------------------\n",
    "Usage: konduit [COMMAND] [OPTIONS] [arg...]\n",
    "\n",
    "Commands:\n",
    "    build         Command line interface for performing Konduit Serving builds.\n",
    "    config        A helper command for creating boiler plate json/yaml for\n",
    "                  inference configuration\n",
    "    inspect       Inspect the details of a particular konduit server.\n",
    "    list          Lists the running konduit servers.\n",
    "    logs          View the logs of a particular konduit server\n",
    "    predict       Run inference on konduit servers using given inputs\n",
    "    profile       Command to List, view, edit, create and delete konduit serving\n",
    "                  run profiles.\n",
    "    pythonpaths   A utility command to manage system installed and manually\n",
    "                  registered python binaries.\n",
    "    serve         Start a konduit server application\n",
    "    stop          Stop a running konduit server\n",
    "    version       Displays konduit-serving version.\n",
    "\n",
    "Run 'konduit COMMAND --help' for more information on a command.\n",
    "-------------------------------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "Each command describes its short hand description right in front of it. If you want to look at an individual command in detail, you can use the corresponding `--help` command with them. For example, the help menu for the `logs` command can be seen by executing, `konduit logs --help`:\n",
    "\n",
    "```bash\n",
    "$ konduit logs --help\n",
    "-------------------------------------------------------------------------------------------------\n",
    "Usage: konduit logs  [-f] [-l <value>]  server-id\n",
    "\n",
    "View the logs of a particular konduit server\n",
    "\n",
    "View the logs of a particular konduit server given an id.\n",
    "\n",
    "Example usages:\n",
    "--------------\n",
    "- Outputs the log file contents of server with an id of 'inf_server':\n",
    "$ konduit logs inf_server\n",
    "\n",
    "- Outputs and tail the log file contents of server with an id of 'inf_server':\n",
    "$ konduit logs inf_server -f\n",
    "\n",
    "- Outputs and tail the log file contents of server with an id of 'inf_server'\n",
    "  from the last 10 lines:\n",
    "$ konduit logs inf_server -l 10 -f\n",
    "--------------\n",
    "\n",
    "Options and Arguments:\n",
    " -f,--follow          Follow the logs output.\n",
    " -l,--lines <value>   Sets the number of lines to be printed. Default is '10'.\n",
    "                      Use -1 for outputting everything.\n",
    "\n",
    " <server-id>          Konduit server id\n",
    "-------------------------------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "As you can see, the `--help` command for an individual help command describes its functionality in detail along with some explicit examples and use cases. It also describes each individual optional/non-optional argument that can be used with it. This can come in very handy while learning about konduit-serving for the first time and is a useful starting place to play around with a specific command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add jar ../../konduit.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ai.konduit.serving.WebAppEndpoint"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package ai.konduit.serving;\n",
    "\n",
    "import ai.konduit.serving.endpoint.Endpoint;\n",
    "\n",
    "import io.vertx.core.Handler;\n",
    "import io.vertx.core.http.HttpMethod;\n",
    "import io.vertx.ext.web.RoutingContext;\n",
    "\n",
    "import javax.imageio.ImageIO;\n",
    "import java.awt.image.BufferedImage;\n",
    "import java.io.File;\n",
    "import java.io.IOException;\n",
    "import java.util.ArrayList;\n",
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "import ai.konduit.serving.pipeline.api.pipeline.PipelineExecutor;\n",
    "\n",
    "import java.util.Timer;\n",
    "import java.util.TimerTask;\n",
    "import io.vertx.core.http.HttpHeaders;\n",
    "\n",
    "import java.io.File;\n",
    "\n",
    "import java.nio.charset.StandardCharsets;\n",
    "import org.apache.commons.io.FileUtils;\n",
    "import io.vertx.core.http.HttpHeaders;\n",
    "import io.vertx.ext.web.handler.StaticHandler;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class WebAppEndpoint implements Endpoint {\n",
    "    \n",
    "    final static Logger logger = LoggerFactory.getLogger(WebAppEndpoint.class);\n",
    "\n",
    "    private PipelineExecutor pipelineExecutor;\n",
    "\n",
    "    public WebAppEndpoint(PipelineExecutor pipelineExecutor) {\n",
    "        this.pipelineExecutor = pipelineExecutor;\n",
    "    }\n",
    "\n",
    "    public HttpMethod type() { return HttpMethod.GET; }\n",
    "\n",
    "    public String path() { return \"/web-app/*\"; }\n",
    "\n",
    "    public List<String> consumes() { return Arrays.asList(); }\n",
    "\n",
    "    public List<String> produces() { return Arrays.asList(\"application/html\"); }\n",
    "\n",
    "    @Override\n",
    "    public Handler<RoutingContext> handler() {\n",
    "        return handler -> { \n",
    "            try {\n",
    "                logger.info(new File(handler.request().path().substring(1)).getAbsolutePath());\n",
    "                handler.response().sendFile(new File(handler.request().path().substring(1)).getAbsolutePath()).end(); \n",
    "            } catch(Exception e) {\n",
    "                e.printStackTrace();\n",
    "                logger.error(\"Error: \", e);\n",
    "            }\n",
    "        };\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ai.konduit.serving.PrometheusEndpoint"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package ai.konduit.serving;\n",
    "\n",
    "import ai.konduit.serving.endpoint.Endpoint;\n",
    "\n",
    "import io.vertx.core.Handler;\n",
    "import io.vertx.core.http.HttpMethod;\n",
    "import io.vertx.ext.web.RoutingContext;\n",
    "\n",
    "import javax.imageio.ImageIO;\n",
    "import java.awt.image.BufferedImage;\n",
    "import java.io.File;\n",
    "import java.io.IOException;\n",
    "import java.util.ArrayList;\n",
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "import ai.konduit.serving.pipeline.api.pipeline.PipelineExecutor;\n",
    "\n",
    "import io.micrometer.prometheus.PrometheusConfig;\n",
    "import io.micrometer.core.instrument.MeterRegistry;\n",
    "import io.micrometer.prometheus.PrometheusMeterRegistry;\n",
    "import io.micrometer.core.instrument.Counter;\n",
    "import io.micrometer.core.instrument.binder.jvm.JvmMemoryMetrics;\n",
    "import io.micrometer.core.instrument.binder.system.ProcessorMetrics;\n",
    "\n",
    "import java.util.Timer;\n",
    "import java.util.TimerTask;\n",
    "import io.vertx.core.http.HttpHeaders;\n",
    "\n",
    "public class PrometheusEndpoint implements Endpoint {\n",
    "\n",
    "    public static PrometheusMeterRegistry registry;\n",
    "    public static List<Counter> classCounterIncrement = new ArrayList();\n",
    "    public static List<String> labels = Arrays.asList(\"UnderWeight\", \"Normal_Range\", \"OverWeight\", \"Obese_ClassI\", \"Obese_ClassII\", \"Obese_ClassIII\", \"Obese_ClassIV\");\n",
    "    \n",
    "    static {\n",
    "        registry = new PrometheusMeterRegistry(PrometheusConfig.DEFAULT);\n",
    "        \n",
    "        if (registry != null) {\n",
    "            System.out.println(\"Using metrics registry \" + registry.getClass().getName() + \" for inference\");\n",
    "            new JvmMemoryMetrics().bindTo(registry);\n",
    "            new ProcessorMetrics().bindTo(registry);\n",
    "            \n",
    "            // For scraping GPU metrics\n",
    "            try {\n",
    "                Class<?> gpuMetricsClass = Class.forName(\"ai.konduit.serving.gpu.GpuMetrics\");\n",
    "                Object instance = gpuMetricsClass.newInstance();\n",
    "                gpuMetricsClass.getMethod(\"bindTo\", MeterRegistry.class).invoke(instance, registry);\n",
    "            } catch(Exception exception) {\n",
    "                System.out.println(\"No GPU binaries found. Selecting and scraping only CPU metrics.\");\n",
    "            }\n",
    "            \n",
    "            Counter serverUpTimeCounter = registry.counter(\"bmi-onnx-pytorch.server.up.time\");\n",
    "            double increment = 5.0;\n",
    "            new Timer().schedule(new TimerTask() {\n",
    "                @Override\n",
    "                public void run() {\n",
    "                    serverUpTimeCounter.increment(increment);\n",
    "                }\n",
    "            }, 5000, ((int) increment) * 1000);\n",
    "            \n",
    "            \n",
    "            for (String label : labels) {\n",
    "                classCounterIncrement.add(Counter.builder(label)\n",
    "                        .description(\"Classification counts seen so far for class label: \" + label)\n",
    "                        .baseUnit(\"bmi-onnx-pytorch.classification.outcome\")\n",
    "                        .register(registry));\n",
    "            }\n",
    "        } else {\n",
    "            System.out.println(\"Not using metrics registry.\");\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    private PipelineExecutor pipelineExecutor;\n",
    "\n",
    "    public PrometheusEndpoint(PipelineExecutor pipelineExecutor) { \n",
    "        this.pipelineExecutor = pipelineExecutor;\n",
    "    }\n",
    "\n",
    "    public HttpMethod type() { return HttpMethod.GET; }\n",
    "\n",
    "    public String path() { return \"/server-metrics\"; }\n",
    "\n",
    "    public List<String> consumes() { return Arrays.asList(); }\n",
    "\n",
    "    public List<String> produces() { return Arrays.asList(\"text/plain; version=0.0.4; charset=utf-8\"); }\n",
    "\n",
    "    @Override\n",
    "    public Handler<RoutingContext> handler() {\n",
    "        return handler -> handler.response().putHeader(HttpHeaders.CONTENT_TYPE, \"text/plain; version=0.0.4; charset=utf-8\").end(registry.scrape());\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ai.konduit.serving.OCREndPoint"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package ai.konduit.serving;\n",
    "\n",
    "import ai.konduit.serving.endpoint.Endpoint;\n",
    "import ai.konduit.serving.pipeline.api.data.Data;\n",
    "import ai.konduit.serving.pipeline.api.data.Image;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.Pipeline;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.PipelineExecutor;\n",
    "import ai.konduit.serving.pipeline.impl.format.JavaImageFactory;\n",
    "import ai.konduit.serving.pipeline.registry.ImageFactoryRegistry;\n",
    "import io.vertx.core.Handler;\n",
    "import io.vertx.core.http.HttpMethod;\n",
    "import io.vertx.ext.web.RoutingContext;\n",
    "\n",
    "import javax.imageio.ImageIO;\n",
    "import java.awt.image.BufferedImage;\n",
    "import java.io.File;\n",
    "import java.io.IOException;\n",
    "import java.util.ArrayList;\n",
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "import io.micrometer.core.instrument.Counter;\n",
    "import io.micrometer.core.instrument.Gauge;\n",
    "\n",
    "import ai.konduit.serving.pipeline.util.ObjectMappers;\n",
    "import ai.konduit.serving.pipeline.registry.NDArrayConverterRegistry;\n",
    "import ai.konduit.serving.data.nd4j.format.ND4JConverters;\n",
    "import io.vertx.core.json.JsonObject;\n",
    "\n",
    "public class OCREndPoint implements Endpoint {\n",
    "\n",
    "    private PipelineExecutor pipelineExecutor;\n",
    "\n",
    "    private double requestTime = -1.0;\n",
    "    private double pipelineTime = -1.0;\n",
    "\n",
    "    private Counter requestsHandled = PrometheusEndpoint.registry.counter(\"bmi-onnx-pytorch.requests.handled\");\n",
    "    private Gauge requestTimeGuage = Gauge.builder(\"bmi-onnx-pytorch.request.time.ms\", () -> requestTime).register(PrometheusEndpoint.registry);\n",
    "    private Gauge pipelineTimeGuage = Gauge.builder(\"bmi-onnx-pytorch.pipeline.time.ms\", () -> pipelineTime).register(PrometheusEndpoint.registry);\n",
    "    private Gauge requestThroughputGuage = Gauge.builder(\"bmi-onnx-pytorch.request.time.ms\", () -> 1 / requestTime * 1000).register(PrometheusEndpoint.registry);\n",
    "\n",
    "    public OCREndPoint(PipelineExecutor pipelineExecutor) { \n",
    "        this.pipelineExecutor = pipelineExecutor; \n",
    "        ImageFactoryRegistry.addFactory(new JavaImageFactory()); \n",
    "        NDArrayConverterRegistry.addConverter(new ND4JConverters.Nd4jToSerializedConverter()); \n",
    "        NDArrayConverterRegistry.addConverter(new ND4JConverters.SerializedToNd4jArrConverter());\n",
    "    }\n",
    "\n",
    "    public HttpMethod type() { return HttpMethod.POST; }\n",
    "\n",
    "    public String path() { return \"/infer\"; }\n",
    "\n",
    "    public List<String> consumes() { return Arrays.asList(\"application/octet-stream\",\"multipart/form-data\"); }\n",
    "\n",
    "    public List<String> produces() { return Arrays.asList(\"application/json\"); }\n",
    "\n",
    "    @Override\n",
    "    public Handler<RoutingContext> handler() {\n",
    "        return handler -> {\n",
    "            handler.vertx().executeBlocking(taskHandler -> {\n",
    "                double requestTimeStart = (double) System.currentTimeMillis();\n",
    "                Data image = Data.empty();\n",
    "                \n",
    "                try {\n",
    "                    image.put(\"image\",Image.create(ImageIO.read(new File(handler.fileUploads().iterator().next().uploadedFileName()))));\n",
    "                \n",
    "                    double pipelineTimeStart = (double) System.currentTimeMillis();\n",
    "                    Data exec = pipelineExecutor.exec(image);\n",
    "                    double pipelineTimeEnd = (double) System.currentTimeMillis();\n",
    "                    pipelineTime = pipelineTimeEnd - pipelineTimeStart;\n",
    "                    \n",
    "                    handler.response().end(ObjectMappers.toJson(exec));\n",
    "                    taskHandler.complete();\n",
    "\n",
    "                    requestsHandled.increment();\n",
    "\n",
    "                    String bmiClass = exec.getString(\"bmi_class\");\n",
    "                    int index = PrometheusEndpoint.labels.indexOf(bmiClass.replace(\" \", \"_\").trim());\n",
    "                    System.out.format(\"BMI CLASS: %s, for index %s\", bmiClass, index);\n",
    "                    if(index != -1) {\n",
    "                        PrometheusEndpoint.classCounterIncrement.get(index).increment();\n",
    "                    }\n",
    "                    \n",
    "                    double requestTimeEnd = (double) System.currentTimeMillis();\n",
    "                    requestTime = requestTimeEnd - requestTimeStart;\n",
    "                } catch (IOException e) {\n",
    "                    e.printStackTrace();\n",
    "                    handler.response().setStatusCode(500).end(new JsonObject().put(\"error\", e.getMessage()).encode());\n",
    "                    taskHandler.complete();\n",
    "                    \n",
    "                }\n",
    "            },resultHandler -> {\n",
    "                if(resultHandler.failed()) {\n",
    "                    if(resultHandler.cause() != null)\n",
    "                        if(handler.vertx().exceptionHandler() != null)\n",
    "                            handler.vertx().exceptionHandler().handle(resultHandler.cause());\n",
    "                        else {\n",
    "                            resultHandler.cause().printStackTrace();\n",
    "                        }\n",
    "                    else {\n",
    "                        System.err.println(\"Failed to process classification endpoint async task. Unknown cause.\");\n",
    "                    }\n",
    "                }\n",
    "            });\n",
    "\n",
    "        };\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ai.konduit.serving.OCREndPoints"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package ai.konduit.serving;\n",
    "\n",
    "import ai.konduit.serving.endpoint.Endpoint;\n",
    "import ai.konduit.serving.endpoint.HttpEndpoints;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.Pipeline;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.PipelineExecutor;\n",
    "\n",
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "public class OCREndPoints implements HttpEndpoints {\n",
    "    \n",
    "    @Override\n",
    "    public List<Endpoint> endpoints(Pipeline pipeline, PipelineExecutor pipelineExecutor) {\n",
    "        return Arrays.asList(new OCREndPoint(pipelineExecutor), new PrometheusEndpoint(pipelineExecutor), new WebAppEndpoint(pipelineExecutor));\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/beaker6746870317869033181/outDir\n",
      "/root/konduit/konduit.jar\n",
      "\n",
      "-------------\n",
      "Saved content:\n",
      "-------------\n",
      "/tmp/beaker6746870317869033181/outDir:/root/konduit/konduit.jar\n",
      "\tin file:\n",
      "/root/konduit/demos/6-bmi-onnx-pytorch/classpath\n",
      "-------------"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.net.URLClassLoader;\n",
    "import java.net.URL;\n",
    "import java.io.File;\n",
    "\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;\n",
    "\n",
    "import org.apache.commons.io.FileUtils;\n",
    "import java.io.IOException;\n",
    "\n",
    "import java.nio.charset.StandardCharsets;\n",
    "\n",
    "URL[] urls = ((URLClassLoader) Class.forName(\"ai.konduit.serving.vertx.config.InferenceConfiguration\").getClassLoader()).getURLs();\n",
    "List<String> classpaths = new ArrayList<>();\n",
    "\n",
    "for(URL url : urls) {\n",
    "    String singleClassPath = new File(url.toURI()).getAbsolutePath();\n",
    "    System.out.println(singleClassPath);\n",
    "    classpaths.add(singleClassPath);\n",
    "}\n",
    "\n",
    "try {\n",
    "    String output = String.join(File.pathSeparator, classpaths);\n",
    "    File classpathOutputPath = new File(\"classpath\");\n",
    "    FileUtils.writeStringToFile(new File(\"classpath\"), output, StandardCharsets.UTF_8);\n",
    "    System.out.format(\"%n-------------%nSaved content:%n-------------%n%s%n\\tin file:%n%s%n-------------\", output, classpathOutputPath.getAbsolutePath());\n",
    "} catch (IOException e) {\n",
    "    e.printStackTrace();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nohup java -cp $(cat classpath) ai.konduit.serving.cli.launcher.KonduitServingLauncher serve -id bmi-onnx-pytorch -c bmi-onnx-pytorch.yaml -rwm &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:26:02.228 [main] INFO  a.k.s.c.l.command.KonduitRunCommand - Processing configuration: /root/konduit/demos/6-bmi-onnx-pytorch/bmi-onnx-pytorch.yaml\n",
      "00:26:02.257 [main] INFO  u.o.l.s.context.SysOutOverSLF4J - Replaced standard System.out and System.err PrintStreams with SLF4JPrintStreams\n",
      "00:26:02.264 [main] INFO  u.o.l.s.context.SysOutOverSLF4J - Redirected System.out and System.err to SLF4J for this context\n",
      "00:26:02.265 [main] INFO  a.k.s.c.l.command.KonduitRunCommand - Starting konduit server with an id of 'bmi-onnx-pytorch'\n",
      "00:26:03.516 [vert.x-worker-thread-0] INFO  a.k.s.p.registry.PipelineRegistry - Loaded 28 PipelineStepRunnerFactory instances\n",
      "00:26:03.947 [vert.x-worker-thread-0] INFO  a.k.serving.python.PythonRunner - Over riding python path :/root/miniconda/lib/python37.zip:/root/miniconda/lib/python3.7:/root/miniconda/lib/python3.7/lib-dynload:/root/miniconda/lib/python3.7/site-packages\n",
      "00:26:06.942 [vert.x-worker-thread-0] INFO  a.k.serving.python.PythonRunner - Resolving execution code from run_script.py\n",
      "00:26:06.945 [vert.x-worker-thread-0] INFO  a.k.serving.python.PythonRunner - Resolving import code from init_script.py\n",
      "00:26:06.945 [vert.x-worker-thread-0] INFO  org.nd4j.python4j.PythonGIL - Pre Gil State ensure for thread 16\n",
      "00:26:06.945 [vert.x-worker-thread-0] INFO  org.nd4j.python4j.PythonGIL - Thread 16 acquired GIL\n",
      "00:26:07.934 [vert.x-worker-thread-0] INFO  org.nd4j.python4j.PythonGIL - Releasing GIL on thread 16\n",
      "00:26:07.934 [vert.x-worker-thread-0] INFO  a.k.s.v.verticle.InferenceVerticle - \n",
      "\n",
      "####################################################################\n",
      "#                                                                  #\n",
      "#    |  /   _ \\   \\ |  _ \\  |  | _ _| __ __|    |  /     |  /      #\n",
      "#    . <   (   | .  |  |  | |  |   |     |      . <      . <       #\n",
      "#   _|\\_\\ \\___/ _|\\_| ___/ \\__/  ___|   _|     _|\\_\\ _) _|\\_\\ _)   #\n",
      "#                                                                  #\n",
      "####################################################################\n",
      "\n",
      "00:26:07.934 [vert.x-worker-thread-0] INFO  a.k.s.v.verticle.InferenceVerticle - Pending server start, please wait...\n",
      "00:26:07.967 [vert.x-eventloop-thread-0] INFO  a.k.s.v.p.h.v.InferenceVerticleHttp - MetricsProvider implementation detected, adding endpoint /metrics\n",
      "00:26:08.049 [vert.x-eventloop-thread-0] INFO  a.konduit.serving.PrometheusEndpoint - Using metrics registry io.micrometer.prometheus.PrometheusMeterRegistry for inference\n",
      "00:26:08.118 [vert.x-eventloop-thread-0] INFO  a.konduit.serving.PrometheusEndpoint - No GPU binaries found. Selecting and scraping only CPU metrics.\n",
      "00:26:08.458 [vert.x-eventloop-thread-0] INFO  a.k.s.v.verticle.InferenceVerticle - Writing inspection data at '/root/.konduit-serving/servers/134.data' with configuration: \n",
      "{\n",
      "  \"host\" : \"0.0.0.0\",\n",
      "  \"port\" : 9009,\n",
      "  \"useSsl\" : false,\n",
      "  \"protocol\" : \"HTTP\",\n",
      "  \"kafkaConfiguration\" : {\n",
      "    \"startHttpServerForKafka\" : true,\n",
      "    \"httpKafkaHost\" : \"localhost\",\n",
      "    \"httpKafkaPort\" : 0,\n",
      "    \"consumerTopicName\" : \"inference-in\",\n",
      "    \"consumerKeyDeserializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectDeserializer\",\n",
      "    \"consumerValueDeserializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectDeserializer\",\n",
      "    \"consumerGroupId\" : \"konduit-serving-consumer-group\",\n",
      "    \"consumerAutoOffsetReset\" : \"earliest\",\n",
      "    \"consumerAutoCommit\" : \"true\",\n",
      "    \"producerTopicName\" : \"inference-out\",\n",
      "    \"producerKeySerializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectSerializer\",\n",
      "    \"producerValueSerializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectSerializer\",\n",
      "    \"producerAcks\" : \"1\"\n",
      "  },\n",
      "  \"mqttConfiguration\" : { },\n",
      "  \"customEndpoints\" : [ \"ai.konduit.serving.OCREndPoints\" ],\n",
      "  \"pipeline\" : {\n",
      "    \"steps\" : [ {\n",
      "      \"@type\" : \"PYTHON\",\n",
      "      \"pythonConfig\" : {\n",
      "        \"pythonConfigType\" : \"CONDA\",\n",
      "        \"pythonPath\" : \"1\",\n",
      "        \"environmentName\" : \"base\",\n",
      "        \"appendType\" : \"BEFORE\",\n",
      "        \"pythonPathResolution\" : \"STATIC\",\n",
      "        \"pythonCodePath\" : \"run_script.py\",\n",
      "        \"pythonLibrariesPath\" : \":/root/miniconda/lib/python37.zip:/root/miniconda/lib/python3.7:/root/miniconda/lib/python3.7/lib-dynload:/root/miniconda/lib/python3.7/site-packages\",\n",
      "        \"importCodePath\" : \"init_script.py\",\n",
      "        \"pythonInputs\" : { },\n",
      "        \"pythonOutputs\" : { },\n",
      "        \"extraInputs\" : { },\n",
      "        \"returnAllInputs\" : false,\n",
      "        \"setupAndRun\" : false,\n",
      "        \"ioInputs\" : {\n",
      "          \"image\" : {\n",
      "            \"pythonType\" : \"image\",\n",
      "            \"secondaryType\" : \"NONE\",\n",
      "            \"type\" : \"IMAGE\"\n",
      "          }\n",
      "        },\n",
      "        \"ioOutputs\" : {\n",
      "          \"bmi_value\" : {\n",
      "            \"pythonType\" : \"float\",\n",
      "            \"secondaryType\" : \"NONE\",\n",
      "            \"type\" : \"DOUBLE\"\n",
      "          },\n",
      "          \"bmi_class\" : {\n",
      "            \"pythonType\" : \"str\",\n",
      "            \"secondaryType\" : \"NONE\",\n",
      "            \"type\" : \"STRING\"\n",
      "          },\n",
      "          \"boxes\" : {\n",
      "            \"pythonType\" : \"list\",\n",
      "            \"secondaryType\" : \"DOUBLE\",\n",
      "            \"type\" : \"LIST\"\n",
      "          }\n",
      "        },\n",
      "        \"jobSuffix\" : \"konduit_job\"\n",
      "      }\n",
      "    } ]\n",
      "  }\n",
      "}\n",
      "00:26:08.459 [vert.x-eventloop-thread-0] INFO  a.k.s.v.p.h.v.InferenceVerticleHttp - Inference HTTP server is listening on host: '0.0.0.0'\n",
      "00:26:08.459 [vert.x-eventloop-thread-0] INFO  a.k.s.v.p.h.v.InferenceVerticleHttp - Inference HTTP server started on port 9009 with 1 pipeline steps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit logs bmi-onnx-pytorch -l 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Listing konduit servers...\n",
      "\n",
      " #   | ID                             | TYPE       | URL                  | PID     | STATUS     \n",
      " 1   | bmi-onnx-pytorch               | inference  | 0.0.0.0:9009         | 134     | started    \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><img src=\"image_me.jpg\"/></html>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"image_me.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit inspect bmi-onnx-pytorch -q {port}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"bmi_value\" : 22.18,\n",
      "  \"bmi_class\" : \"Normal_Range\",\n",
      "  \"boxes\" : [ 447.0, 174.0, 636.0, 470.0 ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -s -H \"Content-Type: multipart/form-data\" -X POST -F \"image=@image_me.jpg\" http://localhost:$(konduit inspect bmi-onnx-pytorch -q {port})/infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "The cell below also embeds the associated metrics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<div style=\"display: flex; justify-content: center; align-items: center; border: 1px solid black;\">\n",
       "    <iframe src=\"http://localhost:3000/d/lP_JcnHWz/pipeline-metrics?orgId=1&refresh=5s&kiosk&var-serverName=bmi_onnx_pytorch\" width=1500 height=1300>\n",
       "</div></html>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; border: 1px solid black;\">\n",
    "    <iframe src=\"http://localhost:3000/d/lP_JcnHWz/pipeline-metrics?orgId=1&refresh=5s&kiosk&var-serverName=bmi_onnx_pytorch\" width=1500 height=1300>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web application\n",
    "The cell below demonstrate the web application served by konduit-serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><div style=\"display: flex; justify-content: center; align-items: center; border: 1px solid black;\">\n",
       "    <iframe src=\"http://localhost:9009/web-app/index.html\" allow=\"camera;microphone\", width=1000 height=1000></iframe>\n",
       "</div></html>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; border: 1px solid black;\">\n",
    "    <iframe src=\"http://localhost:9009/web-app/index.html\" allow=\"camera;microphone\", width=1000 height=1000></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping konduit server 'bmi-onnx-pytorch'\n",
      "Application 'bmi-onnx-pytorch' terminated with status 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit stop bmi-onnx-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View metrics in Browser\n",
    "Visit: http://localhost:3000/d/lP_JcnHWz/pipeline-metrics?orgId=1&refresh=5s&kiosk&var-serverName=tensorflow_mnist to view metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "text/x-java",
   "file_extension": ".java",
   "mimetype": "",
   "name": "Java",
   "nbconverter_exporter": "",
   "version": "1.8.0_121"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
