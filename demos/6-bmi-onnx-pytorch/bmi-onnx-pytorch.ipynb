{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konduit Serving BMI Calculator Setup\n",
    "Body mass index (BMI) is a measure of body fat based on height and weight. The standard way of calculating BMI is through the formula below:\n",
    "\n",
    "$$ \\text{BMI}=\\frac{m} {{h}^2} $$\n",
    "\n",
    "$$ \\text{BMI} = \\text{Body Mass Index} $$\n",
    "\n",
    "$$ \\text{m} = \\text{mass (in kilograms/pounds)} $$\n",
    "\n",
    "$$ \\text{h} = \\text{height (in meters/feet)} $$\n",
    "\n",
    "### A new approach for estimating Body Mass Index using facial features with Konduit-Serving\n",
    "Measuring weight and height of individual people is time-consuming while also being error prone. In this notebook, we'll see how we can measure BMI of an individual just by looking at their facial features. The backend server used for taking image data and providing BMI values is served using Konduit-Serving, which is a high performance model pipeline server. Model training, gathering and preparing dataset is out of the scope of this notebook. The main workflow we'll look at is how to serve a model that can look at a person's face and answer back with a BMI value through REST API. We'll also be setting up a web server through konduit-serving \"custom endpoints\" that will make use of a webcam and label the canvas with the detected face along with the corresponding estimated BMI value. \n",
    "\n",
    "So, let's get started!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konduit Serving Components\n",
    "\n",
    "Before diving deep into the coding part, let's look at some of the few konduit-serving components in detail. \n",
    "\n",
    "### 1. CLI Interface\n",
    "Konduit-Serving comes with a CLI interface (with a `konduit` alias) that's responsible of taking care of most aspects of the application. The help command will describe most of what we are able to do with konduit-serving. Executing `konduit --help` command will show us the following output:\n",
    "\n",
    "```bash\n",
    "$ konduit --help\n",
    "-------------------------------------------------------------------------------------------------\n",
    "Usage: konduit [COMMAND] [OPTIONS] [arg...]\n",
    "\n",
    "Commands:\n",
    "    build         Command line interface for performing Konduit Serving builds.\n",
    "    config        A helper command for creating boiler plate json/yaml for\n",
    "                  inference configuration\n",
    "    inspect       Inspect the details of a particular konduit server.\n",
    "    list          Lists the running konduit servers.\n",
    "    logs          View the logs of a particular konduit server\n",
    "    predict       Run inference on konduit servers using given inputs\n",
    "    profile       Command to List, view, edit, create and delete konduit serving\n",
    "                  run profiles.\n",
    "    pythonpaths   A utility command to manage system installed and manually\n",
    "                  registered python binaries.\n",
    "    serve         Start a konduit server application\n",
    "    stop          Stop a running konduit server\n",
    "    version       Displays konduit-serving version.\n",
    "\n",
    "Run 'konduit COMMAND --help' for more information on a command.\n",
    "-------------------------------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "Each command describes its short hand description right in front of it. If you want to look at an individual command in detail, you can use the corresponding `--help` command with them. For example, the help menu for the `logs` command can be seen by executing, `konduit logs --help`:\n",
    "\n",
    "```bash\n",
    "$ konduit logs --help\n",
    "-------------------------------------------------------------------------------------------------\n",
    "Usage: konduit logs  [-f] [-l <value>]  server-id\n",
    "\n",
    "View the logs of a particular konduit server\n",
    "\n",
    "View the logs of a particular konduit server given an id.\n",
    "\n",
    "Example usages:\n",
    "--------------\n",
    "- Outputs the log file contents of server with an id of 'inf_server':\n",
    "$ konduit logs inf_server\n",
    "\n",
    "- Outputs and tail the log file contents of server with an id of 'inf_server':\n",
    "$ konduit logs inf_server -f\n",
    "\n",
    "- Outputs and tail the log file contents of server with an id of 'inf_server'\n",
    "  from the last 10 lines:\n",
    "$ konduit logs inf_server -l 10 -f\n",
    "--------------\n",
    "\n",
    "Options and Arguments:\n",
    " -f,--follow          Follow the logs output.\n",
    " -l,--lines <value>   Sets the number of lines to be printed. Default is '10'.\n",
    "                      Use -1 for outputting everything.\n",
    "\n",
    " <server-id>          Konduit server id\n",
    "-------------------------------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "As you can see, the `--help` command for an individual help command describes its functionality in detail along with some explicit examples and use cases. It also describes each individual optional/non-optional argument that can be used with it. This can come in very handy while learning about konduit-serving for the first time and is a useful starting place to play around with a specific command. You can do the same for the rest of the commands. Which are: `build, config, inspect, list, logs, predict, profile, pythonpaths, serve, stop, version`\n",
    "\n",
    "### 2. Konduit-Serving JAR file\n",
    "Each konduit-serving distribution whether it is for Windows, Linux or MacOS comes contained in a JAR file. So, you'll need a Java Virtual Machine present in the system where you're using Konduit-Serving as a Model Pipeline Server. The CLI itself is linked with the jar file and utilizes a java runtime internally to interact with the Konduit-Serving package. If you look at the konduit serving distribution, you'll see the following folder architecture in the root folder:\n",
    "\n",
    "konduit (root folder) \\\n",
    "├── **bin** (directory) \\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;└── **konduit** (konduit serving CLI script) \\\n",
    "├── **conf** (directory) \\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;└── **konduit-serving-env.sh** (for configuration of different environment variables for konduit-serving) \\\n",
    "└── **konduit.jar** (main package JAR file) \n",
    "\n",
    "The main CLI logic is places under `bin/konduit` file. If you see the contents of that file, you'll see the following output:\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "SCRIPT_DIR=\"$(dirname \"$0\")\"\n",
    "\n",
    ". ${SCRIPT_DIR}/../conf/konduit-serving-env.sh\n",
    "\n",
    "java -jar -Dvertx.cli.usage.prefix=konduit ${SCRIPT_DIR}/../konduit.jar \"$@\"\n",
    "```\n",
    "\n",
    "As you can see, it uses the `java` command which is available through a Java runtime environment. The java command itself uses the `konduit.jar` file which is the main application package inside a Konduit-Serving distribution.\n",
    "\n",
    "This JAR file will be uses as a Java application dependency while creating custom endpoints logic for a Konduit-Serving pipeline. We'll get to how we can do that later in this notebook.\n",
    "\n",
    "### What is a Model Serving Pipeline?\n",
    "Throughout in this notebook we have used the term \"Model Serving Pipeline\". This refers to how Machine Learning or Deep Learning models get served on an application server. Machine and Deep Learning models work on n-dimensional arrays (also knows as ND-Arrays). They don't know how to convert a JPEG or PNG image into numbers directly. Instead, they expect pre-processed data in the form of multi-dimensional array. Also, any other form of data, be it text, audio or video, gets converted into numbers ND-Arrays before getting fed into a machine learning model.\n",
    "\n",
    "The process during which the data is converted from one form to another is called pre-processing and is done just before it's fed as a model input. So, in a sense you can see this as being Lego blocks fitting into each other. One part takes input in a specific form and outputs it into another form, which in turn gets fed into the next part. This chaining of processes creates a series of steps which have specific jobs to perform before the next step and the end result is a machine learning Pipeline. The typical flow of the pipeline looks like the following: \n",
    "\n",
    "(1) Input Image in JPEG -> (2) Converts JPEG to ND-Array -> (3) Model -> (4) Output\n",
    "\n",
    "A pipeline can also be in a form of a directed acyclic graph or DAG where data can flow into the graph and can give multiple outputs. In Konduit-Serving a Pipeline graph can also contain optional graph branches and can also concatenate outputs from multiple graph nodes. For the sake of current goal (BMI Model Serving) we'll stick to a Sequential Pipeline which only has one input and one output.\n",
    "\n",
    "#### Pipeline Step\n",
    "Inside Konduit-Serving, a pipeline can be broken down into steps, where each step is responsible for performing a specific function. A pipeline step is the smallest component of a whole pipeline and can be used for a whole list of operations. To see the list of available pipeline steps you can use the `config` command in Konduit-Serving CLI.\n",
    "\n",
    "```bash\n",
    "$ konduit config --help\n",
    "-------------------------------------------------------------------------------------------------\n",
    "Usage: konduit config  [-m] [-o <output-file>] -p <config> [-pr <value>]  [-y]\n",
    "\n",
    "A helper command for creating boiler plate json/yaml for inference configuration\n",
    "\n",
    "This command is a utility to create boilerplate json/yaml configurations that\n",
    "can be conveniently modified to start konduit servers.\n",
    "\n",
    "Example usages:\n",
    "--------------\n",
    "                     -- FOR SEQUENCE PIPELINES--\n",
    "- Prints 'logging -> tensorflow -> logging' config in pretty format:\n",
    "$ konduit config -p logging,tensorflow,logging\n",
    "\n",
    "- Prints 'logging -> tensorflow -> logging' config with gRPC protocol\n",
    "  in pretty format:\n",
    "$ konduit config -p logging,tensorflow,logging -pr grpc\n",
    "\n",
    "- Prints 'dl4j -> logging' config in minified format:\n",
    "$ konduit config -p dl4j,logging -m\n",
    "\n",
    "- Saves 'dl4j -> logging' config in a 'config.json' file:\n",
    "$ konduit config -p dl4j,logging -o config.json\n",
    "\n",
    "- Saves 'dl4j -> logging' config in a 'config.yaml' file:\n",
    "$ konduit config -p dl4j,logging -y -o config.json\n",
    "\n",
    "\n",
    "                  -- FOR GRAPH PIPELINES --\n",
    "- Generates a config that logs the input(1) then flow them through two\n",
    "  tensorflow models(2,3) and merges the output(4):\n",
    "$ konduit config -p\n",
    "1=logging(input),2=tensorflow(1),3=tensorflow(1),4=merge(2,3)\n",
    "\n",
    "- Generates a config that logs the input(1) then channels(2) them through one\n",
    "  of the two tensorflow models(3,4) and then selects the output(5) based\n",
    "  on the value of the selection integer field 'select'\n",
    "$ konduit config -p\n",
    "1=logging(input),[2_1,2_2]=switch(int,select,1),3=tensorflow(2_1),4=tensorflow(2\n",
    "_2),5=any(3,4)\n",
    "\n",
    "- Generates a config that logs the input(1) then channels(2) them through one\n",
    "  of the two tensorflow models(3,4) and then selects the output(5) based\n",
    "  on the value of the selection string field 'select' in the selection map\n",
    "  (x:0,y:1).\n",
    "$ konduit config -p\n",
    "1=logging(input),[2_1,2_2]=switch(string,select,x:0,y:1,1),3=tensorflow(2_1),4=t\n",
    "ensorflow(2_2),5=any(3,4)\n",
    "--------------\n",
    "\n",
    "Options and Arguments:\n",
    " -m,--minified               If set, the output json will be printed in a single\n",
    "                             line, without indentations. (Ignored for yaml\n",
    "                             configuration output)\n",
    " -o,--output <output-file>   Optional: If set, the generated json/yaml will be\n",
    "                             saved here. Otherwise, it's printed on the console.\n",
    " -p,--pipeline <config>      A comma-separated list of sequence/graph pipeline\n",
    "                             steps to create boilerplate configuration from. For\n",
    "                             sequences, allowed values are: [crop_grid,\n",
    "                             crop_fixed_grid, dl4j, keras, draw_bounding_box,\n",
    "                             draw_fixed_grid, draw_grid, draw_segmentation,\n",
    "                             extract_bounding_box, camera_frame_capture,\n",
    "                             video_frame_capture, image_to_ndarray, logging,\n",
    "                             ssd_to_bounding_box, samediff, show_image,\n",
    "                             tensorflow, nd4jtensorflow, python, onnx]. For\n",
    "                             graphs, the list item should be in the format\n",
    "                             '<output>=<type>(<inputs>)' or\n",
    "                             '[outputs]=switch(<inputs>)' for switches. The\n",
    "                             pre-defined root input is named, 'input'. Examples\n",
    "                             are ==> Pipeline step:\n",
    "                             'a=tensorflow(input),b=dl4j(input)' Merge Step:\n",
    "                             'c=merge(a,b)' Switch Step (int):\n",
    "                             '[d1,d2,d3]=switch(int,select,input)' Switch Step\n",
    "                             (string):\n",
    "                             '[d1,d2,d3]=switch(string,select,x:1,y:2,z:3,input)\n",
    "                             'Any Step: 'e=any(d1,d2,d3)' See the examples above\n",
    "                             for more usage information.\n",
    " -pr,--protocol <value>      Protocol to use with the server. Allowed values are\n",
    "                             [http, grpc, mqtt]\n",
    " -y,--yaml                   Set if you want the output to be a yaml\n",
    "                             configuration.\n",
    "-------------------------------------------------------------------------------------------------\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%classpath add jar ../../konduit.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping konduit server 'bmi-onnx-pytorch'\n",
      "Application 'bmi-onnx-pytorch' terminated with status 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit stop bmi-onnx-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ai.konduit.serving.WebAppEndpoint"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package ai.konduit.serving;\n",
    "\n",
    "import ai.konduit.serving.endpoint.Endpoint;\n",
    "\n",
    "import io.vertx.core.Handler;\n",
    "import io.vertx.core.http.HttpMethod;\n",
    "import io.vertx.ext.web.RoutingContext;\n",
    "\n",
    "import javax.imageio.ImageIO;\n",
    "import java.awt.image.BufferedImage;\n",
    "import java.io.File;\n",
    "import java.io.IOException;\n",
    "import java.util.ArrayList;\n",
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "import ai.konduit.serving.pipeline.api.pipeline.PipelineExecutor;\n",
    "\n",
    "import java.util.Timer;\n",
    "import java.util.TimerTask;\n",
    "import io.vertx.core.http.HttpHeaders;\n",
    "\n",
    "import java.io.File;\n",
    "\n",
    "import java.nio.charset.StandardCharsets;\n",
    "import org.apache.commons.io.FileUtils;\n",
    "import io.vertx.core.http.HttpHeaders;\n",
    "import io.vertx.ext.web.handler.StaticHandler;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class WebAppEndpoint implements Endpoint {\n",
    "    \n",
    "    final static Logger logger = LoggerFactory.getLogger(WebAppEndpoint.class);\n",
    "\n",
    "    private PipelineExecutor pipelineExecutor;\n",
    "\n",
    "    public WebAppEndpoint(PipelineExecutor pipelineExecutor) {\n",
    "        this.pipelineExecutor = pipelineExecutor;\n",
    "    }\n",
    "\n",
    "    public HttpMethod type() { return HttpMethod.GET; }\n",
    "\n",
    "    public String path() { return \"/web-app/*\"; }\n",
    "\n",
    "    public List<String> consumes() { return Arrays.asList(); }\n",
    "\n",
    "    public List<String> produces() { return Arrays.asList(\"application/html\"); }\n",
    "\n",
    "    @Override\n",
    "    public Handler<RoutingContext> handler() {\n",
    "        return handler -> { \n",
    "            try {\n",
    "                logger.info(new File(handler.request().path().substring(1)).getAbsolutePath());\n",
    "                handler.response().sendFile(new File(handler.request().path().substring(1)).getAbsolutePath()).end(); \n",
    "            } catch(Exception e) {\n",
    "                e.printStackTrace();\n",
    "                logger.error(\"Error: \", e);\n",
    "            }\n",
    "        };\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ai.konduit.serving.CustomEndpoints"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package ai.konduit.serving;\n",
    "\n",
    "import ai.konduit.serving.endpoint.Endpoint;\n",
    "import ai.konduit.serving.endpoint.HttpEndpoints;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.Pipeline;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.PipelineExecutor;\n",
    "\n",
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "public class CustomEndpoints implements HttpEndpoints {\n",
    "    \n",
    "    @Override\n",
    "    public List<Endpoint> endpoints(Pipeline pipeline, PipelineExecutor pipelineExecutor) {\n",
    "        return Arrays.asList(new WebAppEndpoint(pipelineExecutor));\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/beaker8984958766066872621/outDir\n",
      "/root/konduit/konduit.jar\n",
      "\n",
      "-------------\n",
      "Saved content:\n",
      "-------------\n",
      "/tmp/beaker8984958766066872621/outDir:/root/konduit/konduit.jar\n",
      "\tin file:\n",
      "/root/konduit/demos/6-bmi-onnx-pytorch/classpath\n",
      "-------------"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.net.URLClassLoader;\n",
    "import java.net.URL;\n",
    "import java.io.File;\n",
    "\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;\n",
    "\n",
    "import org.apache.commons.io.FileUtils;\n",
    "import java.io.IOException;\n",
    "\n",
    "import java.nio.charset.StandardCharsets;\n",
    "\n",
    "URL[] urls = ((URLClassLoader) Class.forName(\"ai.konduit.serving.vertx.config.InferenceConfiguration\").getClassLoader()).getURLs();\n",
    "List<String> classpaths = new ArrayList<>();\n",
    "\n",
    "for(URL url : urls) {\n",
    "    String singleClassPath = new File(url.toURI()).getAbsolutePath();\n",
    "    System.out.println(singleClassPath);\n",
    "    classpaths.add(singleClassPath);\n",
    "}\n",
    "\n",
    "try {\n",
    "    String output = String.join(File.pathSeparator, classpaths);\n",
    "    File classpathOutputPath = new File(\"classpath\");\n",
    "    FileUtils.writeStringToFile(new File(\"classpath\"), output, StandardCharsets.UTF_8);\n",
    "    System.out.format(\"%n-------------%nSaved content:%n-------------%n%s%n\\tin file:%n%s%n-------------\", output, classpathOutputPath.getAbsolutePath());\n",
    "} catch (IOException e) {\n",
    "    e.printStackTrace();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nohup java -cp $(cat classpath) ai.konduit.serving.cli.launcher.KonduitServingLauncher serve -id bmi-onnx-pytorch -c bmi-onnx-pytorch.yaml -rwm &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:16:58.428 [main] INFO  a.k.s.c.l.command.KonduitRunCommand - Processing configuration: /root/konduit/demos/6-bmi-onnx-pytorch/bmi-onnx-pytorch.yaml\n",
      "05:16:58.434 [main] INFO  u.o.l.s.context.SysOutOverSLF4J - Replaced standard System.out and System.err PrintStreams with SLF4JPrintStreams\n",
      "05:16:58.435 [main] INFO  u.o.l.s.context.SysOutOverSLF4J - Redirected System.out and System.err to SLF4J for this context\n",
      "05:16:58.436 [main] INFO  a.k.s.c.l.command.KonduitRunCommand - Starting konduit server with an id of 'bmi-onnx-pytorch'\n",
      "05:16:58.759 [vert.x-worker-thread-0] INFO  a.k.s.p.registry.PipelineRegistry - Loaded 28 PipelineStepRunnerFactory instances\n",
      "05:16:59.121 [vert.x-worker-thread-0] INFO  a.k.serving.python.PythonRunner - Over riding python path :/root/miniconda/lib/python37.zip:/root/miniconda/lib/python3.7:/root/miniconda/lib/python3.7/lib-dynload:/root/miniconda/lib/python3.7/site-packages\n",
      "05:16:59.711 [vert.x-worker-thread-0] INFO  a.k.serving.python.PythonRunner - Resolving execution code from run_script.py\n",
      "05:16:59.711 [vert.x-worker-thread-0] INFO  a.k.serving.python.PythonRunner - Resolving import code from init_script.py\n",
      "05:16:59.711 [vert.x-worker-thread-0] INFO  org.nd4j.python4j.PythonGIL - Pre Gil State ensure for thread 17\n",
      "05:16:59.711 [vert.x-worker-thread-0] INFO  org.nd4j.python4j.PythonGIL - Thread 17 acquired GIL\n",
      "05:17:01.712 [vert.x-worker-thread-0] INFO  org.nd4j.python4j.PythonGIL - Releasing GIL on thread 17\n",
      "05:17:01.713 [vert.x-worker-thread-0] INFO  a.k.s.v.verticle.InferenceVerticle - \n",
      "\n",
      "####################################################################\n",
      "#                                                                  #\n",
      "#    |  /   _ \\   \\ |  _ \\  |  | _ _| __ __|    |  /     |  /      #\n",
      "#    . <   (   | .  |  |  | |  |   |     |      . <      . <       #\n",
      "#   _|\\_\\ \\___/ _|\\_| ___/ \\__/  ___|   _|     _|\\_\\ _) _|\\_\\ _)   #\n",
      "#                                                                  #\n",
      "####################################################################\n",
      "\n",
      "05:17:01.713 [vert.x-worker-thread-0] INFO  a.k.s.v.verticle.InferenceVerticle - Pending server start, please wait...\n",
      "05:17:01.735 [vert.x-eventloop-thread-0] INFO  a.k.s.v.p.h.v.InferenceVerticleHttp - MetricsProvider implementation detected, adding endpoint /metrics\n",
      "05:17:01.874 [vert.x-eventloop-thread-0] INFO  a.k.s.v.verticle.InferenceVerticle - Writing inspection data at '/root/.konduit-serving/servers/41181.data' with configuration: \n",
      "{\n",
      "  \"host\" : \"0.0.0.0\",\n",
      "  \"port\" : 9009,\n",
      "  \"useSsl\" : false,\n",
      "  \"protocol\" : \"HTTP\",\n",
      "  \"kafkaConfiguration\" : {\n",
      "    \"startHttpServerForKafka\" : true,\n",
      "    \"httpKafkaHost\" : \"localhost\",\n",
      "    \"httpKafkaPort\" : 0,\n",
      "    \"consumerTopicName\" : \"inference-in\",\n",
      "    \"consumerKeyDeserializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectDeserializer\",\n",
      "    \"consumerValueDeserializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectDeserializer\",\n",
      "    \"consumerGroupId\" : \"konduit-serving-consumer-group\",\n",
      "    \"consumerAutoOffsetReset\" : \"earliest\",\n",
      "    \"consumerAutoCommit\" : \"true\",\n",
      "    \"producerTopicName\" : \"inference-out\",\n",
      "    \"producerKeySerializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectSerializer\",\n",
      "    \"producerValueSerializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectSerializer\",\n",
      "    \"producerAcks\" : \"1\"\n",
      "  },\n",
      "  \"mqttConfiguration\" : { },\n",
      "  \"customEndpoints\" : [ \"ai.konduit.serving.CustomEndpoints\" ],\n",
      "  \"pipeline\" : {\n",
      "    \"steps\" : [ {\n",
      "      \"@type\" : \"PYTHON\",\n",
      "      \"pythonConfig\" : {\n",
      "        \"pythonConfigType\" : \"CONDA\",\n",
      "        \"pythonPath\" : \"1\",\n",
      "        \"environmentName\" : \"base\",\n",
      "        \"appendType\" : \"BEFORE\",\n",
      "        \"pythonPathResolution\" : \"STATIC\",\n",
      "        \"pythonCodePath\" : \"run_script.py\",\n",
      "        \"pythonLibrariesPath\" : \":/root/miniconda/lib/python37.zip:/root/miniconda/lib/python3.7:/root/miniconda/lib/python3.7/lib-dynload:/root/miniconda/lib/python3.7/site-packages\",\n",
      "        \"importCodePath\" : \"init_script.py\",\n",
      "        \"pythonInputs\" : { },\n",
      "        \"pythonOutputs\" : { },\n",
      "        \"extraInputs\" : { },\n",
      "        \"returnAllInputs\" : false,\n",
      "        \"setupAndRun\" : false,\n",
      "        \"ioInputs\" : {\n",
      "          \"image\" : {\n",
      "            \"pythonType\" : \"image\",\n",
      "            \"secondaryType\" : \"NONE\",\n",
      "            \"type\" : \"IMAGE\"\n",
      "          }\n",
      "        },\n",
      "        \"ioOutputs\" : {\n",
      "          \"bmi_value\" : {\n",
      "            \"pythonType\" : \"float\",\n",
      "            \"secondaryType\" : \"NONE\",\n",
      "            \"type\" : \"DOUBLE\"\n",
      "          },\n",
      "          \"boxes\" : {\n",
      "            \"pythonType\" : \"list\",\n",
      "            \"secondaryType\" : \"DOUBLE\",\n",
      "            \"type\" : \"LIST\"\n",
      "          },\n",
      "          \"predictions\" : {\n",
      "            \"pythonType\" : \"numpy.ndarray\",\n",
      "            \"secondaryType\" : \"DOUBLE\",\n",
      "            \"type\" : \"NDARRAY\"\n",
      "          }\n",
      "        },\n",
      "        \"jobSuffix\" : \"konduit_job\"\n",
      "      }\n",
      "    }, {\n",
      "      \"@type\" : \"CLASSIFIER_OUTPUT\",\n",
      "      \"inputName\" : \"predictions\",\n",
      "      \"returnLabel\" : true,\n",
      "      \"returnIndex\" : true,\n",
      "      \"returnProb\" : true,\n",
      "      \"labelName\" : \"bmi_class\",\n",
      "      \"indexName\" : \"index\",\n",
      "      \"probName\" : \"prob\",\n",
      "      \"labels\" : [ \"UnderWeight\", \"Normal_Range\", \"OverWeight\", \"Obese_ClassI\", \"Obese_ClassII\", \"Obese_ClassIII\", \"Obese_ClassIV\", \"None\" ],\n",
      "      \"allProbabilities\" : false\n",
      "    } ]\n",
      "  }\n",
      "}\n",
      "05:17:01.874 [vert.x-eventloop-thread-0] INFO  a.k.s.v.p.h.v.InferenceVerticleHttp - Inference HTTP server is listening on host: '0.0.0.0'\n",
      "05:17:01.874 [vert.x-eventloop-thread-0] INFO  a.k.s.v.p.h.v.InferenceVerticleHttp - Inference HTTP server started on port 9009 with 2 pipeline steps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit logs bmi-onnx-pytorch -l 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Listing konduit servers...\n",
      "\n",
      " #   | ID                             | TYPE       | URL                  | PID     | STATUS     \n",
      " 1   | bmi-onnx-pytorch               | inference  | 0.0.0.0:9009         | 41181   | started    \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><img src=\"image_me.jpg\"/></html>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"image_me.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit inspect bmi-onnx-pytorch -q {port}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"bmi_value\" : 22.18,\n",
      "  \"boxes\" : [ 447.0, 174.0, 636.0, 470.0 ],\n",
      "  \"predictions\" : {\n",
      "    \"@NDArrayType\" : \"DOUBLE\",\n",
      "    \"@NDArrayShape\" : [ 8 ],\n",
      "    \"@NDArrayDataBase64\" : \"AAAAAAAAAAAAAAAAAADwPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"\n",
      "  },\n",
      "  \"prob\" : 1.0,\n",
      "  \"index\" : 1,\n",
      "  \"bmi_class\" : \"Normal_Range\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -s -H \"Content-Type: multipart/form-data\" -H \"Accept: application/json\" -X POST -F \"image=@image_me.jpg\" http://localhost:9009/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "The cell below also embeds the associated metrics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<div style=\"display: flex; justify-content: center; align-items: center; border: 1px solid black;\">\n",
       "    <iframe src=\"http://localhost:3000/d/lP_JcnHWz/pipeline-metrics?orgId=1&refresh=5s&kiosk&var-serverName=bmi-onnx-pytorch\" width=1500 height=1300>\n",
       "</div></html>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; border: 1px solid black;\">\n",
    "    <iframe src=\"http://localhost:3000/d/lP_JcnHWz/pipeline-metrics?orgId=1&refresh=5s&kiosk&var-serverName=bmi-onnx-pytorch\" width=1500 height=1300>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web application\n",
    "The cell below demonstrate the web application served by konduit-serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><div style=\"display: flex; justify-content: center; align-items: center; border: 1px solid black;\">\n",
       "    <iframe src=\"http://localhost:9009/web-app/index.html\" allow=\"camera;microphone\", width=1000 height=1000></iframe>\n",
       "</div></html>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; border: 1px solid black;\">\n",
    "    <iframe src=\"http://localhost:9009/web-app/index.html\" allow=\"camera;microphone\", width=1000 height=1000></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View metrics in Browser\n",
    "Visit: http://localhost:3000/d/lP_JcnHWz/pipeline-metrics?orgId=1&refresh=5s&kiosk&var-serverName=bmi-onnx-pytorch to view metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "text/x-java",
   "file_extension": ".java",
   "mimetype": "",
   "name": "Java",
   "nbconverter_exporter": "",
   "version": "1.8.0_121"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
