{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running and IRIS dataset classifier through CUSTOM endpoints\n",
    "---\n",
    "## Adding package to the classpath\n",
    "First of all we need to add the main package to the classpath so that the notebook can load all the necessary libraries from konduit-serving into the Jupyter notebook kernel.\n",
    "\n",
    "Classpaths can be considered similar to `site-packages` in the python ecosystem where each library that's to be imported to your code is loaded from.\n",
    "\n",
    "We package almost everything you need to get started with the `konduit.jar` package so you can just start working on the actual code, without having to care about any boilerplate configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cb8a37-110f-4d9c-911c-9f04573790aa",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add jar ../../konduit.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory /root/konduit/demos/1-pytorch-onnx-iris\n",
      ".\n",
      "├── dataset\n",
      "│   └── iris.csv\n",
      "├── iris.onnx\n",
      "├── onnx-iris.ipynb\n",
      "├── onnx.yaml\n",
      "└── train.py\n",
      "\n",
      "1 directory, 5 files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"Current directory $(pwd)\" && tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main model script code\n",
    "We're creating a pytorch model from scratch here and then converting that into ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.autograd import Variable\n",
      "\n",
      "\n",
      "class Net(nn.Module):\n",
      "    # define nn\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.fc1 = nn.Linear(4, 100)\n",
      "        self.fc2 = nn.Linear(100, 100)\n",
      "        self.fc3 = nn.Linear(100, 3)\n",
      "        self.softmax = nn.Softmax(dim=1)\n",
      "\n",
      "    def forward(self, X):\n",
      "        X = F.relu(self.fc1(X))\n",
      "        X = self.fc2(X)\n",
      "        X = self.fc3(X)\n",
      "        X = self.softmax(X)\n",
      "\n",
      "        return X\n",
      "\n",
      "\n",
      "# load IRIS dataset\n",
      "dataset = pd.read_csv('dataset/iris.csv')\n",
      "\n",
      "# transform species to numerics\n",
      "dataset.loc[dataset.species == 'Iris-setosa', 'species'] = 0\n",
      "dataset.loc[dataset.species == 'Iris-versicolor', 'species'] = 1\n",
      "dataset.loc[dataset.species == 'Iris-virginica', 'species'] = 2\n",
      "\n",
      "train_X, test_X, train_y, test_y = train_test_split(dataset[dataset.columns[0:4]].values,\n",
      "                                                    dataset.species.values, test_size=0.8)\n",
      "\n",
      "# wrap up with Variable in pytorch\n",
      "train_X = Variable(torch.Tensor(train_X).float())\n",
      "test_X = Variable(torch.Tensor(test_X).float())\n",
      "train_y = Variable(torch.Tensor(train_y).long())\n",
      "test_y = Variable(torch.Tensor(test_y).long())\n",
      "\n",
      "net = Net()\n",
      "\n",
      "criterion = nn.CrossEntropyLoss()  # cross entropy loss\n",
      "\n",
      "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
      "\n",
      "for epoch in range(1000):\n",
      "    optimizer.zero_grad()\n",
      "    out = net(train_X)\n",
      "    loss = criterion(out, train_y)\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "    if epoch % 100 == 0:\n",
      "        print('number of epoch', epoch, 'loss', loss.item())\n",
      "\n",
      "predict_out = net(test_X)\n",
      "_, predict_y = torch.max(predict_out, 1)\n",
      "\n",
      "print('prediction accuracy', accuracy_score(test_y.data, predict_y.data))\n",
      "\n",
      "print('macro precision', precision_score(test_y.data, predict_y.data, average='macro'))\n",
      "print('micro precision', precision_score(test_y.data, predict_y.data, average='micro'))\n",
      "print('macro recall', recall_score(test_y.data, predict_y.data, average='macro'))\n",
      "print('micro recall', recall_score(test_y.data, predict_y.data, average='micro'))\n",
      "\n",
      "# Input to the model\n",
      "x = torch.randn(1, 4, requires_grad=True)\n",
      "\n",
      "# Export the model\n",
      "torch.onnx.export(net,  # model being run\n",
      "                  x,  # model input (or a tuple for multiple inputs)\n",
      "                  \"iris.onnx\",  # where to save the model (can be a file or file-like object)\n",
      "                  export_params=True,  # store the trained parameter weights inside the model file\n",
      "                  opset_version=10,  # the ONNX version to export the model to\n",
      "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
      "                  input_names=['input'],  # the model's input names\n",
      "                  output_names=['output'],  # the model's output names\n",
      "                  dynamic_axes={'input': {0: 'batch_size'},  # variable length axes\n",
      "                                'output': {0: 'batch_size'}})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "less train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the configuration file\n",
    "The configuration for the custom endpoint is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "custom_endpoints:\n",
      "  - \"ai.konduit.IrisEndPoints\"\n",
      "host: \"localhost\"\n",
      "port: 0\n",
      "protocol: \"HTTP\"\n",
      "pipeline:\n",
      "  steps:\n",
      "  - '@type': \"ONNX\"\n",
      "    modelUri: \"iris.onnx\"\n",
      "    inputNames:\n",
      "    - \"input\"\n",
      "    outputNames:\n",
      "    - \"output\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "less onnx.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example input data\n",
    "The example input data that we'll send to the server will contain the main 4 parameters of a class, such as: \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"sepal_length\":5.1,\n",
    "    \"sepal_width\":3.5,\n",
    "    \"petal_length\":1.4,\n",
    "    \"petal_width\":0.2\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a customized endpoint for custom inferences\n",
    "We can write our own code for a custom endpoint that will take care of the inputs in the way we specify exactly and also send the output as well like it to. A simple example of processing `JSON` input is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ai.konduit.IrisEndPoint"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package ai.konduit;\n",
    "\n",
    "import ai.konduit.serving.endpoint.Endpoint;\n",
    "import ai.konduit.serving.pipeline.api.data.Data;\n",
    "import ai.konduit.serving.pipeline.api.data.Image;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.Pipeline;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.PipelineExecutor;\n",
    "import io.vertx.core.Handler;\n",
    "import io.vertx.core.http.HttpMethod;\n",
    "import io.vertx.ext.web.RoutingContext;\n",
    "\n",
    "import javax.imageio.ImageIO;\n",
    "import java.awt.image.BufferedImage;\n",
    "import java.io.File;\n",
    "import java.io.IOException;\n",
    "import java.util.ArrayList;\n",
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "import org.nd4j.linalg.factory.Nd4j;\n",
    "import ai.konduit.serving.pipeline.api.data.NDArray;\n",
    "import ai.konduit.serving.pipeline.util.ObjectMappers;\n",
    "\n",
    "import ai.konduit.serving.pipeline.registry.NDArrayConverterRegistry;\n",
    "import ai.konduit.serving.data.nd4j.format.ND4JConverters;\n",
    "    \n",
    "public class IrisEndPoint implements Endpoint {\n",
    "\n",
    "    private PipelineExecutor pipelineExecutor;\n",
    "\n",
    "    public IrisEndPoint(PipelineExecutor pipelineExecutor) { \n",
    "        this.pipelineExecutor = pipelineExecutor; \n",
    "        NDArrayConverterRegistry.addConverter(new ND4JConverters.Nd4jToSerializedConverter()); \n",
    "        NDArrayConverterRegistry.addConverter(new ND4JConverters.SerializedToNd4jArrConverter()); }\n",
    "\n",
    "    public HttpMethod type() { return HttpMethod.POST; }\n",
    "\n",
    "    public String path() { return \"/infer\"; }\n",
    "\n",
    "    public List<String> consumes() { return Arrays.asList(\"application/json\"); }\n",
    "\n",
    "    public List<String> produces() { return Arrays.asList(\"application/json\"); }\n",
    "\n",
    "    @Override\n",
    "    public Handler<RoutingContext> handler() {\n",
    "        return handler -> {\n",
    "            handler.vertx().executeBlocking(taskHandler -> {\n",
    "                Data data = Data.empty();\n",
    "                \n",
    "                try {\n",
    "                    data.put(\"input\", NDArray.create(Nd4j.create(new float[] {\n",
    "                        handler.getBodyAsJson().getFloat(\"sepal_length\"),\n",
    "                        handler.getBodyAsJson().getFloat(\"sepal_width\"),\n",
    "                        handler.getBodyAsJson().getFloat(\"petal_length\"),\n",
    "                        handler.getBodyAsJson().getFloat(\"petal_width\")}, new int[] { 1, 4 })));\n",
    "                } catch (Exception e) {\n",
    "                    e.printStackTrace();\n",
    "                }\n",
    "\n",
    "                Data exec = pipelineExecutor.exec(data);\n",
    "                \n",
    "                handler.response().end(ObjectMappers.toJson(exec.getNDArray(\"output\").getAs(float[].class)));\n",
    "                taskHandler.complete();\n",
    "            },resultHandler -> {\n",
    "                if(resultHandler.failed()) {\n",
    "                    if(resultHandler.cause() != null)\n",
    "                        if(handler.vertx().exceptionHandler() != null)\n",
    "                            handler.vertx().exceptionHandler().handle(resultHandler.cause());\n",
    "                        else {\n",
    "                            resultHandler.cause().printStackTrace();\n",
    "                        }\n",
    "                    else {\n",
    "                        System.err.println(\"Failed to process classification endpoint async task. Unknown cause.\");\n",
    "                    }\n",
    "                }\n",
    "            });\n",
    "\n",
    "        };\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping all the created endpoints\n",
    "Now we can wrap all the created endpoints into a wrapper class and specify it in the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ai.konduit.IrisEndPoints"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package ai.konduit;\n",
    "\n",
    "import ai.konduit.serving.endpoint.Endpoint;\n",
    "import ai.konduit.serving.endpoint.HttpEndpoints;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.Pipeline;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.PipelineExecutor;\n",
    "\n",
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "public class IrisEndPoints implements HttpEndpoints {\n",
    "\n",
    "    @Override\n",
    "    public List<Endpoint> endpoints(Pipeline pipeline, PipelineExecutor pipelineExecutor) {\n",
    "        return Arrays.asList(new IrisEndPoint(pipelineExecutor));\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/beaker5183223511681744326/outDir\n",
      "/root/konduit/konduit.jar\n",
      "Saved /tmp/beaker5183223511681744326/outDir:/root/konduit/konduit.jar at: /root/konduit/demos/1-pytorch-onnx-iris/classpath\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.net.URLClassLoader;\n",
    "import java.net.URL;\n",
    "import java.io.File;\n",
    "\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;\n",
    "\n",
    "import org.apache.commons.io.FileUtils;\n",
    "import java.io.IOException;\n",
    "\n",
    "import java.nio.charset.StandardCharsets;\n",
    "\n",
    "URL[] urls = ((URLClassLoader) Class.forName(\"ai.konduit.serving.vertx.config.InferenceConfiguration\").getClassLoader()).getURLs();\n",
    "List<String> classpaths = new ArrayList<>();\n",
    "\n",
    "for(URL url : urls) {\n",
    "    String singleClassPath = new File(url.toURI()).getAbsolutePath();\n",
    "    System.out.println(singleClassPath);\n",
    "    classpaths.add(singleClassPath);\n",
    "}\n",
    "\n",
    "try {\n",
    "    String output = String.join(File.pathSeparator, classpaths);\n",
    "    File classpathOutputPath = new File(\"classpath\");\n",
    "    FileUtils.writeStringToFile(new File(\"classpath\"), output, StandardCharsets.UTF_8);\n",
    "    System.out.format(\"Saved %s at: %s%n\", output, classpathOutputPath.getAbsolutePath());\n",
    "} catch (IOException e) {\n",
    "    e.printStackTrace();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing Stored Compiled Classes Structure\n",
    "The following command shows how the classes are stored in the `/tmp` directory by BeakerX in their respective package directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/beaker5183223511681744326/outDir\n",
      "├── ai\n",
      "│   └── konduit\n",
      "│       ├── IrisEndPoint.class\n",
      "│       └── IrisEndPoints.class\n",
      "└── com\n",
      "    └── twosigma\n",
      "        └── beaker\n",
      "            └── javash\n",
      "                └── bkr70705afc\n",
      "                    └── BeakerWrapperClass1261714175Id0af29d9b6262497bac5049c56fb86f77.class\n",
      "\n",
      "7 directories, 3 files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tree /tmp/beaker*/outDir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting konduit server...\n",
      "Using classpath: /tmp/beaker5183223511681744326/outDir:/root/konduit/konduit.jar\n",
      "INFO: Running command /root/miniconda/jre/bin/java -Dkonduit.logs.file.path=/root/.konduit-serving/command_logs/onnx-iris.log -Dlogback.configurationFile=/tmp/logback-run_command_7022826e6a574042.xml ai.konduit.serving.cli.launcher.KonduitServingLauncher run --instances 1 -s inference -c onnx.yaml -Dserving.id=onnx-iris\n",
      "For server status, execute: 'java ai.konduit.serving.cli.launcher.KonduitServingLauncher list'\n",
      "For logs, execute: 'java ai.konduit.serving.cli.launcher.KonduitServingLauncher logs onnx-iris'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "java -cp $(cat classpath) ai.konduit.serving.cli.launcher.KonduitServingLauncher serve -id onnx-iris -c onnx.yaml -rwm -b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:47:39.240 [main] INFO  a.k.s.c.l.command.KonduitRunCommand - Processing configuration: /root/konduit/demos/1-pytorch-onnx-iris/onnx.yaml\n",
      "00:47:39.246 [main] INFO  u.o.l.s.context.SysOutOverSLF4J - Replaced standard System.out and System.err PrintStreams with SLF4JPrintStreams\n",
      "00:47:39.247 [main] INFO  u.o.l.s.context.SysOutOverSLF4J - Redirected System.out and System.err to SLF4J for this context\n",
      "00:47:39.248 [main] INFO  a.k.s.c.l.command.KonduitRunCommand - Starting konduit server with an id of 'onnx-iris'\n",
      "00:47:39.560 [vert.x-worker-thread-0] INFO  a.k.s.p.registry.PipelineRegistry - Loaded 28 PipelineStepRunnerFactory instances\n",
      "00:47:39.834 [vert.x-worker-thread-0] INFO  a.k.s.v.verticle.InferenceVerticle - \n",
      "\n",
      "####################################################################\n",
      "#                                                                  #\n",
      "#    |  /   _ \\   \\ |  _ \\  |  | _ _| __ __|    |  /     |  /      #\n",
      "#    . <   (   | .  |  |  | |  |   |     |      . <      . <       #\n",
      "#   _|\\_\\ \\___/ _|\\_| ___/ \\__/  ___|   _|     _|\\_\\ _) _|\\_\\ _)   #\n",
      "#                                                                  #\n",
      "####################################################################\n",
      "\n",
      "00:47:39.834 [vert.x-worker-thread-0] INFO  a.k.s.v.verticle.InferenceVerticle - Pending server start, please wait...\n",
      "00:47:39.844 [vert.x-eventloop-thread-0] INFO  a.k.s.v.p.h.v.InferenceVerticleHttp - MetricsProvider implementation detected, adding endpoint /metrics\n",
      "00:47:39.939 [vert.x-eventloop-thread-0] INFO  a.k.s.v.verticle.InferenceVerticle - Writing inspection data at '/root/.konduit-serving/servers/1296.data' with configuration: \n",
      "{\n",
      "  \"host\" : \"localhost\",\n",
      "  \"port\" : 42245,\n",
      "  \"useSsl\" : false,\n",
      "  \"protocol\" : \"HTTP\",\n",
      "  \"kafkaConfiguration\" : {\n",
      "    \"startHttpServerForKafka\" : true,\n",
      "    \"httpKafkaHost\" : \"localhost\",\n",
      "    \"httpKafkaPort\" : 0,\n",
      "    \"consumerTopicName\" : \"inference-in\",\n",
      "    \"consumerKeyDeserializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectDeserializer\",\n",
      "    \"consumerValueDeserializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectDeserializer\",\n",
      "    \"consumerGroupId\" : \"konduit-serving-consumer-group\",\n",
      "    \"consumerAutoOffsetReset\" : \"earliest\",\n",
      "    \"consumerAutoCommit\" : \"true\",\n",
      "    \"producerTopicName\" : \"inference-out\",\n",
      "    \"producerKeySerializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectSerializer\",\n",
      "    \"producerValueSerializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectSerializer\",\n",
      "    \"producerAcks\" : \"1\"\n",
      "  },\n",
      "  \"mqttConfiguration\" : { },\n",
      "  \"customEndpoints\" : [ \"ai.konduit.IrisEndPoints\" ],\n",
      "  \"pipeline\" : {\n",
      "    \"steps\" : [ {\n",
      "      \"@type\" : \"ONNX\",\n",
      "      \"modelUri\" : \"iris.onnx\",\n",
      "      \"inputNames\" : [ \"input\" ],\n",
      "      \"outputNames\" : [ \"output\" ]\n",
      "    } ]\n",
      "  }\n",
      "}\n",
      "00:47:39.940 [vert.x-eventloop-thread-0] INFO  a.k.s.v.p.h.v.InferenceVerticleHttp - Inference HTTP server is listening on host: 'localhost'\n",
      "00:47:39.940 [vert.x-eventloop-thread-0] INFO  a.k.s.v.p.h.v.InferenceVerticleHttp - Inference HTTP server started on port 42245 with 1 pipeline steps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit logs onnx-iris -l 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the running port from the created server\n",
    "The server can be inspected for configuration details with the `konduit inspect` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit inspect onnx-iris -q {port}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending inputs\n",
    "Now we can send our inputs through `cURL` for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99312085, 0.0068791825, 6.1220806E-9 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -s -H \"Content-Type: application/json\" -X POST --data '{\"sepal_length\":5.1,\"sepal_width\":3.5,\"petal_length\":1.4,\"petal_width\":0.2}' http://localhost:$(konduit inspect onnx-iris -q {port})/infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the server\n",
    "Now after we're done with the server, we can stop it through the `konduit stop` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping konduit server 'onnx-iris'\n",
      "Application 'onnx-iris' terminated with status 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit stop onnx-iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "text/x-java",
   "file_extension": ".java",
   "mimetype": "",
   "name": "Java",
   "nbconverter_exporter": "",
   "version": "1.8.0_121"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
