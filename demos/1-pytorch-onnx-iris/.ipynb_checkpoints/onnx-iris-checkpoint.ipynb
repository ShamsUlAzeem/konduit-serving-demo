{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running and IRIS dataset classifier through CUSTOM endpoints\n",
    "---\n",
    "## Adding package to the classpath\n",
    "First of all we need to add the main package to the classpath so that the notebook can load all the necessary libraries from konduit-serving into the Jupyter notebook kernel.\n",
    "\n",
    "Classpaths can be considered similar to `site-packages` in the python ecosystem where each library that's to be imported to your code is loaded from.\n",
    "\n",
    "We package almost everything you need to get started with the `konduit.jar` package so you can just start working on the actual code, without having to care about any boilerplate configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%classpath add jar ../../konduit.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory /root/konduit/demos/1-pytorch-onnx-iris\n",
      ".\n",
      "├── classpath\n",
      "├── dataset\n",
      "│   └── iris.csv\n",
      "├── iris.onnx\n",
      "├── onnx-iris.ipynb\n",
      "├── onnx.yaml\n",
      "└── train.py\n",
      "\n",
      "1 directory, 6 files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"Current directory $(pwd)\" && tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main model script code\n",
    "We're creating a pytorch model from scratch here and then converting that into ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.autograd import Variable\n",
      "\n",
      "\n",
      "class Net(nn.Module):\n",
      "    # define nn\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.fc1 = nn.Linear(4, 100)\n",
      "        self.fc2 = nn.Linear(100, 100)\n",
      "        self.fc3 = nn.Linear(100, 3)\n",
      "        self.softmax = nn.Softmax(dim=1)\n",
      "\n",
      "    def forward(self, X):\n",
      "        X = F.relu(self.fc1(X))\n",
      "        X = self.fc2(X)\n",
      "        X = self.fc3(X)\n",
      "        X = self.softmax(X)\n",
      "\n",
      "        return X\n",
      "\n",
      "\n",
      "# load IRIS dataset\n",
      "dataset = pd.read_csv('dataset/iris.csv')\n",
      "\n",
      "# transform species to numerics\n",
      "dataset.loc[dataset.species == 'Iris-setosa', 'species'] = 0\n",
      "dataset.loc[dataset.species == 'Iris-versicolor', 'species'] = 1\n",
      "dataset.loc[dataset.species == 'Iris-virginica', 'species'] = 2\n",
      "\n",
      "train_X, test_X, train_y, test_y = train_test_split(dataset[dataset.columns[0:4]].values,\n",
      "                                                    dataset.species.values, test_size=0.8)\n",
      "\n",
      "# wrap up with Variable in pytorch\n",
      "train_X = Variable(torch.Tensor(train_X).float())\n",
      "test_X = Variable(torch.Tensor(test_X).float())\n",
      "train_y = Variable(torch.Tensor(train_y).long())\n",
      "test_y = Variable(torch.Tensor(test_y).long())\n",
      "\n",
      "net = Net()\n",
      "\n",
      "criterion = nn.CrossEntropyLoss()  # cross entropy loss\n",
      "\n",
      "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
      "\n",
      "for epoch in range(1000):\n",
      "    optimizer.zero_grad()\n",
      "    out = net(train_X)\n",
      "    loss = criterion(out, train_y)\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "    if epoch % 100 == 0:\n",
      "        print('number of epoch', epoch, 'loss', loss.item())\n",
      "\n",
      "predict_out = net(test_X)\n",
      "_, predict_y = torch.max(predict_out, 1)\n",
      "\n",
      "print('prediction accuracy', accuracy_score(test_y.data, predict_y.data))\n",
      "\n",
      "print('macro precision', precision_score(test_y.data, predict_y.data, average='macro'))\n",
      "print('micro precision', precision_score(test_y.data, predict_y.data, average='micro'))\n",
      "print('macro recall', recall_score(test_y.data, predict_y.data, average='macro'))\n",
      "print('micro recall', recall_score(test_y.data, predict_y.data, average='micro'))\n",
      "\n",
      "# Input to the model\n",
      "x = torch.randn(1, 4, requires_grad=True)\n",
      "\n",
      "# Export the model\n",
      "torch.onnx.export(net,  # model being run\n",
      "                  x,  # model input (or a tuple for multiple inputs)\n",
      "                  \"iris.onnx\",  # where to save the model (can be a file or file-like object)\n",
      "                  export_params=True,  # store the trained parameter weights inside the model file\n",
      "                  opset_version=10,  # the ONNX version to export the model to\n",
      "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
      "                  input_names=['input'],  # the model's input names\n",
      "                  output_names=['output'],  # the model's output names\n",
      "                  dynamic_axes={'input': {0: 'batch_size'},  # variable length axes\n",
      "                                'output': {0: 'batch_size'}})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "less train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the configuration file\n",
    "The configuration for the custom endpoint is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "host: \"localhost\"\n",
      "port: 0\n",
      "protocol: \"HTTP\"\n",
      "pipeline:\n",
      "  steps:\n",
      "  - '@type': \"ONNX\"\n",
      "    modelUri: \"iris.onnx\"\n",
      "    inputNames:\n",
      "    - \"input\"\n",
      "    outputNames:\n",
      "    - \"output\"\n",
      "  - '@type': \"CLASSIFIER_OUTPUT\"\n",
      "    input_name: \"output\"\n",
      "    labels:\n",
      "      - Setosa\n",
      "      - Versicolor\n",
      "      - Virginica\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "less onnx.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting konduit server...\n",
      "Using classpath: /root/konduit/bin/../konduit.jar\n",
      "INFO: Running command /root/miniconda/jre/bin/java -Dkonduit.logs.file.path=/root/.konduit-serving/command_logs/onnx-iris.log -Dlogback.configurationFile=/tmp/logback-run_command_80a3902b721c4c3f.xml -jar /root/konduit/bin/../konduit.jar run --instances 1 -s inference -c onnx.yaml -Dserving.id=onnx-iris\n",
      "For server status, execute: 'konduit list'\n",
      "For logs, execute: 'konduit logs onnx-iris'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit serve -id onnx-iris -c onnx.yaml -rwm -b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:01:50.334 [main] INFO  a.k.s.c.l.command.KonduitRunCommand - Processing configuration: /root/konduit/demos/1-pytorch-onnx-iris/onnx.yaml\n",
      "15:01:50.340 [main] INFO  u.o.l.s.context.SysOutOverSLF4J - Replaced standard System.out and System.err PrintStreams with SLF4JPrintStreams\n",
      "15:01:50.341 [main] INFO  u.o.l.s.context.SysOutOverSLF4J - Redirected System.out and System.err to SLF4J for this context\n",
      "15:01:50.342 [main] INFO  a.k.s.c.l.command.KonduitRunCommand - Starting konduit server with an id of 'onnx-iris'\n",
      "15:01:50.686 [vert.x-worker-thread-0] INFO  a.k.s.p.registry.PipelineRegistry - Loaded 28 PipelineStepRunnerFactory instances\n",
      "15:01:50.794 [vert.x-worker-thread-0] INFO  a.k.s.v.verticle.InferenceVerticle - \n",
      "\n",
      "####################################################################\n",
      "#                                                                  #\n",
      "#    |  /   _ \\   \\ |  _ \\  |  | _ _| __ __|    |  /     |  /      #\n",
      "#    . <   (   | .  |  |  | |  |   |     |      . <      . <       #\n",
      "#   _|\\_\\ \\___/ _|\\_| ___/ \\__/  ___|   _|     _|\\_\\ _) _|\\_\\ _)   #\n",
      "#                                                                  #\n",
      "####################################################################\n",
      "\n",
      "15:01:50.795 [vert.x-worker-thread-0] INFO  a.k.s.v.verticle.InferenceVerticle - Pending server start, please wait...\n",
      "15:01:50.815 [vert.x-eventloop-thread-0] INFO  a.k.s.v.p.h.v.InferenceVerticleHttp - MetricsProvider implementation detected, adding endpoint /metrics\n",
      "15:01:50.832 [vert.x-eventloop-thread-0] INFO  a.k.s.v.p.h.v.InferenceVerticleHttp - No GPU binaries found. Selecting and scraping only CPU metrics.\n",
      "15:01:50.919 [vert.x-eventloop-thread-0] INFO  a.k.s.v.verticle.InferenceVerticle - Writing inspection data at '/root/.konduit-serving/servers/1779.data' with configuration: \n",
      "{\n",
      "  \"host\" : \"localhost\",\n",
      "  \"port\" : 35761,\n",
      "  \"useSsl\" : false,\n",
      "  \"protocol\" : \"HTTP\",\n",
      "  \"staticContentRoot\" : \"static-content\",\n",
      "  \"staticContentUrl\" : \"/static-content\",\n",
      "  \"staticContentIndexPage\" : \"/index.html\",\n",
      "  \"kafkaConfiguration\" : {\n",
      "    \"startHttpServerForKafka\" : true,\n",
      "    \"httpKafkaHost\" : \"localhost\",\n",
      "    \"httpKafkaPort\" : 0,\n",
      "    \"consumerTopicName\" : \"inference-in\",\n",
      "    \"consumerKeyDeserializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectDeserializer\",\n",
      "    \"consumerValueDeserializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectDeserializer\",\n",
      "    \"consumerGroupId\" : \"konduit-serving-consumer-group\",\n",
      "    \"consumerAutoOffsetReset\" : \"earliest\",\n",
      "    \"consumerAutoCommit\" : \"true\",\n",
      "    \"producerTopicName\" : \"inference-out\",\n",
      "    \"producerKeySerializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectSerializer\",\n",
      "    \"producerValueSerializerClass\" : \"io.vertx.kafka.client.serialization.JsonObjectSerializer\",\n",
      "    \"producerAcks\" : \"1\"\n",
      "  },\n",
      "  \"mqttConfiguration\" : { },\n",
      "  \"customEndpoints\" : [ ],\n",
      "  \"pipeline\" : {\n",
      "    \"steps\" : [ {\n",
      "      \"@type\" : \"ONNX\",\n",
      "      \"modelUri\" : \"iris.onnx\",\n",
      "      \"inputNames\" : [ \"input\" ],\n",
      "      \"outputNames\" : [ \"output\" ]\n",
      "    }, {\n",
      "      \"@type\" : \"CLASSIFIER_OUTPUT\",\n",
      "      \"inputName\" : \"output\",\n",
      "      \"returnLabel\" : true,\n",
      "      \"returnIndex\" : true,\n",
      "      \"returnProb\" : true,\n",
      "      \"labelName\" : \"label\",\n",
      "      \"indexName\" : \"index\",\n",
      "      \"probName\" : \"prob\",\n",
      "      \"labels\" : [ \"Setosa\", \"Versicolor\", \"Virginica\" ],\n",
      "      \"allProbabilities\" : false\n",
      "    } ]\n",
      "  }\n",
      "}\n",
      "15:01:50.919 [vert.x-eventloop-thread-0] INFO  a.k.s.v.p.h.v.InferenceVerticleHttp - Inference HTTP server is listening on host: 'localhost'\n",
      "15:01:50.919 [vert.x-eventloop-thread-0] INFO  a.k.s.v.p.h.v.InferenceVerticleHttp - Inference HTTP server started on port 35761 with 2 pipeline steps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit logs onnx-iris -l 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending inputs\n",
    "Now we can send our inputs through `cURL` for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"output\" : [ [ 0.99312085, 0.0068791825, 6.1220806E-9 ] ],\n",
      "  \"prob\" : 0.9931208491325378,\n",
      "  \"index\" : 0,\n",
      "  \"label\" : \"Setosa\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit predict onnx-iris \"{\\\"input\\\":[[5.1,3.5,1.4,0.2]]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"output\" : [ [ 0.99312085, 0.0068791825, 6.1220806E-9 ] ],\n",
      "  \"prob\" : 0.9931208491325378,\n",
      "  \"index\" : 0,\n",
      "  \"label\" : \"Setosa\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit predict onnx-iris --input-type multipart \"input=[[5.1,3.5,1.4,0.2]]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the server\n",
    "Now after we're done with the server, we can stop it through the `konduit stop` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping konduit server 'onnx-iris'\n",
      "Application 'onnx-iris' terminated with status 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit stop onnx-iris"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "text/x-java",
   "file_extension": ".java",
   "mimetype": "",
   "name": "Java",
   "nbconverter_exporter": "",
   "version": "1.8.0_121"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
