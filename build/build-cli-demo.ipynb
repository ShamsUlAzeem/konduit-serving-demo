{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konduit-Serving\n",
    "\n",
    "---\n",
    "\n",
    "## What is konduit-serving?\n",
    "In a nutshell konduit-serving is a system for deploying machine learning models pipelines of nearly any type to production. \n",
    "It also allows embedding Python/Java code (pre/post processing, custom models). It primarily focuses on server and edge deployments using REST and gRPC endpoints.\n",
    "Pipelines are deployed and defined by JSON/YAML or a command line interface.\n",
    "\n",
    "### Konduit-Serving Package Structure\n",
    "The konduit-serving package structure is typically a zip file which contains related files in the following structure\n",
    "\n",
    "├── **bin** (directory) \\\n",
    "│   └── **konduit** (konduit serving CLI script) \\\n",
    "├── **conf** (directory) \\\n",
    "│   └── **konduit-serving-env.sh** (for configuration of different environment variables for konduit-serving) \\\n",
    "└── **konduit.jar** (main package JAR file) \n",
    "\n",
    "## Where does the package files come from?\n",
    "Package file comes from building the main konduit-serving sources from https://github.com/KonduitAI/konduit-serving and then the packaged CLI can be used for building different flavors of konduit-serving packages.\n",
    "\n",
    "###  Packaging\n",
    "\n",
    "We can make custom builds through the konduit-serving `build` cli command. Example of the custom builds are:\n",
    "- HTTP (REST) server on x86 Windows (CPU), packaged as a stand-alone .exe\n",
    "- HTTP and GRPC server on CUDA 10.2 + Linux, packaged as a docker image\n",
    "- (Upcoming) Build konduit-serving binaries for Aurora chip for different model type support\n",
    "\n",
    "Build command is useful for creating minimal packages that can be distributed for different purposes. \n",
    "\n",
    "The typical format for running the `build` command would look like this: \n",
    "`konduit build` \n",
    "\n",
    "```bash\n",
    "$ konduit build --arch x86 --modules python,deeplearning4j --serverType http,grpc --config jar.outputdir=output,jar.name=build-linux.jar --os linux --deploymentType JAR --pipeline config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/konduit/build\n",
      "..\n",
      "├── bin\n",
      "│   └── konduit\n",
      "├── build\n",
      "│   ├── build-cli-demo.ipynb\n",
      "│   └── config.json\n",
      "├── conf\n",
      "│   └── konduit-serving-env.sh\n",
      "├── demos\n",
      "│   ├── 0-python-simple\n",
      "│   │   ├── init_script.py\n",
      "│   │   ├── python-simple.ipynb\n",
      "│   │   ├── python.yaml\n",
      "│   │   └── run_script.py\n",
      "│   ├── 1-pytorch-onnx-iris\n",
      "│   │   ├── classpath\n",
      "│   │   ├── dataset\n",
      "│   │   │   └── iris.csv\n",
      "│   │   ├── iris.onnx\n",
      "│   │   ├── onnx-iris.ipynb\n",
      "│   │   ├── onnx.yaml\n",
      "│   │   └── train.py\n",
      "│   └── 2-pytorch-onnx-mnist\n",
      "│       ├── classpath\n",
      "│       ├── config.json\n",
      "│       ├── mnist.onnx\n",
      "│       ├── onnx-mnist.ipynb\n",
      "│       └── test-image.jpg\n",
      "├── docs.md\n",
      "└── konduit.jar\n",
      "\n",
      "8 directories, 21 files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "pwd && tree .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating $PATH Variable\n",
    "Konduit-serving script should be in the classpaths for it to be discoverable everywhere on a console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin:/opt/conda/condabin:/opt/conda/bin:/root/konduit/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo $PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing konduit command help\n",
    "You can view the help documentation of the `konduit` command using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: konduit [COMMAND] [OPTIONS] [arg...]\n",
      "\n",
      "Commands:\n",
      "    build         Command line interface for performing Konduit Serving builds.\n",
      "    config        A helper command for creating boiler plate json/yaml for\n",
      "                  inference configuration\n",
      "    inspect       Inspect the details of a particular konduit server.\n",
      "    list          Lists the running konduit servers.\n",
      "    logs          View the logs of a particular konduit server\n",
      "    predict       Run inference on konduit servers using given inputs\n",
      "    profile       Command to List, view, edit, create and delete konduit serving\n",
      "                  run profiles.\n",
      "    pythonpaths   A utility command to manage system installed and manually\n",
      "                  registered python binaries.\n",
      "    serve         Start a konduit server application\n",
      "    stop          Stop a running konduit server\n",
      "    version       Displays konduit-serving version.\n",
      "\n",
      "Run 'konduit COMMAND --help' for more information on a command.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `konduit build` command help\n",
    "The `build` command documentation can be viewed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: konduit build [-ad <value>] [-a <value>] [-c <value>]  [-dt <value>] [-d\n",
      "       <value>] [-m <value>] [-o <value>] [-p <value>] [-s <value>]\n",
      "\n",
      "Command line interface for performing Konduit Serving builds.\n",
      "\n",
      "Allows the user to build a JAR or artifact such as a docker image suitable for\n",
      "performing inference on a given pipeline on a given deployment target (defined\n",
      "as an operating system, CPU architecture and optionally compute device). For\n",
      "example, can be used to build for any of the following:\n",
      "-> HTTP (REST) server on x86 Windows (CPU), packaged as a stand-alone .exe\n",
      "-> HTTP and GRPC server on CUDA 10.2 + Linux, packaged as a docker image\n",
      "And many more combinations\n",
      "\n",
      "Example usages:\n",
      "--------------\n",
      "- Creates a deployment for classpath manifest jar for a CPU device:\n",
      "$ konduit build -dt classpath -c classpath.outputFile=manifest.jar\n",
      "  classpath.type=jar_manifest -p pipeline.json -d CPU\n",
      "\n",
      "- Creates a uber jar deployment for a CUDA 10.1 device:\n",
      "$ konduit build -dt classpath -c jar.outputdir=build jar.name=uber.jar\n",
      "  -p pipeline.json -d CUDA_10.1\n",
      "--------------\n",
      "\n",
      "Options and Arguments:\n",
      " -ad,--addDep <value>           Additional dependencies to include, in GAV(C)\n",
      "                                format: \"group_id:artifact_id:version\" /\n",
      "                                \"group_id:artifact_id:version:classifier\"\n",
      " -a,--arch <value>              The target CPU architecture. Must be one of\n",
      "                                {x86, x86_avx2, x86_avx512, armhf, arm64,\n",
      "                                ppc64le}.\n",
      "                                Note that most modern desktops can be built with\n",
      "                                x86_avx2, which is the default\n",
      " -c,--config <value>            Configuration for the deployment types specified\n",
      "                                via -dt/--deploymentType.\n",
      "                                For example, \"-c jar.outputdir=/some/dir\n",
      "                                jar.name=my.jar\" etc.\n",
      "                                Configuration keys:\n",
      "                                JAR deployment config keys: jar.outputdir,\n",
      "                                jar.name,jar.groupid, jar.artifactid,\n",
      "                                jar.version\n",
      "                                ClassPathDeployment config keys:\n",
      "                                classpath.outputFile, classpath.type\n",
      " -dt,--deploymentType <value>   The deployment types to use: JAR, DOCKER, EXE,\n",
      "                                WAR, RPM, DEB or TAR (case insensitive)\n",
      " -d,--device <value>            Compute device to be used. If not set: artifacts\n",
      "                                are build for CPU only.\n",
      "                                Valid values: CPU, CUDA_10.0, CUDA_10.1,\n",
      "                                CUDA_10.2 (case insensitive)\n",
      " -m,--modules <value>           Names of the Konduit Serving modules to include,\n",
      "                                as a comma-separated list of values.\n",
      "                                Note that this is not necessary when a pipeline\n",
      "                                is included (via -p/--pipeline), as the modules\n",
      "                                will be inferred automatically based on the\n",
      "                                pipeline contents\n",
      " -o,--os <value>                Operating systems to build for. Valid values:\n",
      "                                {linux, windows, mac} (case insensitive).\n",
      "                                If not set, the current system OS will be used\n",
      " -p,--pipeline <value>          Path to a pipeline json file\n",
      " -s,--serverType <value>        Type of server - HTTP or GRPC (case insensitive)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "konduit build --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing the `konduit build` command\n",
    "As an example, you can create an `x86` CPU build including the DL4J model (also HTTP + GRPC servers) support for Linux and Windows OS through the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================== Konduit Serving Build Tool ==================================\n",
      "------------------------------------- Build Configuration --------------------------------------\n",
      "Pipeline:                     config.json\n",
      "Target OS:                    [linux, windows]\n",
      "Target CPU arch.:             x86\n",
      "Target Device:                CPU\n",
      "Additional modules:           deeplearning4j\n",
      "Server type(s):               http, grpc\n",
      "Deployment type(s):           JAR\n",
      "Additional dependencies:      \n",
      "\n",
      "\n",
      "Deployment configuration: UberJarDeployment\n",
      "  jar.outputdir:              output\n",
      "  jar.name:                   build-linux-windows.jar\n",
      "  jar.groupid:                null\n",
      "  jar.artifactid:             null\n",
      "  jar.version:                null\n",
      "\n",
      "\n",
      "--------------------------------------- Validating Build ---------------------------------------\n",
      "Resolving modules required for pipeline execution...\n",
      "  konduit-serving-pipeline\n",
      "  konduit-serving-vertx\n",
      "  konduit-serving-cli\n",
      "  konduit-serving-http\n",
      "  konduit-serving-grpc\n",
      "\n",
      "Additional modules specified:\n",
      "  konduit-serving-deeplearning4j\n",
      "\n",
      "Resolving module optional/configurable dependencies for deployment target: Target(LINUX,x86)\n",
      "  konduit-serving-pipeline:          OK\n",
      "  konduit-serving-vertx:             OK\n",
      "  konduit-serving-cli:               OK\n",
      "  konduit-serving-http:              OK\n",
      "  konduit-serving-grpc:              OK\n",
      "  konduit-serving-deeplearning4j:    OK\n",
      "\n",
      "Resolved dependencies:\n",
      "  ai.konduit.serving:konduit-serving-pipeline:0.1.0-SNAPSHOT\n",
      "  ai.konduit.serving:konduit-serving-vertx:0.1.0-SNAPSHOT\n",
      "  ai.konduit.serving:konduit-serving-cli:0.1.0-SNAPSHOT\n",
      "  ai.konduit.serving:konduit-serving-http:0.1.0-SNAPSHOT\n",
      "  ai.konduit.serving:konduit-serving-grpc:0.1.0-SNAPSHOT\n",
      "  ai.konduit.serving:konduit-serving-deeplearning4j:0.1.0-SNAPSHOT\n",
      "  org.nd4j:nd4j-native:1.0.0-beta7\n",
      "  org.nd4j:nd4j-native:1.0.0-beta7:linux-x86_64\n",
      "\n",
      "Checking deployment configurations:\n",
      "  UberJarDeployment:                OK\n",
      "\n",
      ">> Validation Passed\n",
      "\n",
      "-------------------------------------------- Build ---------------------------------------------\n",
      "Generating build files...\n",
      "import com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar\n",
      "repositories {\n",
      "\tmavenCentral()\n",
      "\tmavenLocal()\n",
      "\tjcenter()\n",
      "\tmaven(\"https://oss.sonatype.org/content/repositories/snapshots\")\n",
      "}\n",
      "plugins { java \n",
      "\tid(\"com.github.johnrengelman.shadow\") version \"2.0.4\"\n",
      "\n",
      "}\n",
      "group = \"ai.konduit\"\n",
      "dependencies {\n",
      "\timplementation(\"ai.konduit.serving:konduit-serving-pipeline:0.1.0-SNAPSHOT\")\n",
      "\timplementation(\"ai.konduit.serving:konduit-serving-vertx:0.1.0-SNAPSHOT\")\n",
      "\timplementation(\"ai.konduit.serving:konduit-serving-cli:0.1.0-SNAPSHOT\")\n",
      "\timplementation(\"ai.konduit.serving:konduit-serving-http:0.1.0-SNAPSHOT\")\n",
      "\timplementation(\"ai.konduit.serving:konduit-serving-grpc:0.1.0-SNAPSHOT\")\n",
      "\timplementation(\"ai.konduit.serving:konduit-serving-deeplearning4j:0.1.0-SNAPSHOT\")\n",
      "\timplementation(\"org.nd4j:nd4j-native:1.0.0-beta7\")\n",
      "\timplementation(\"org.nd4j:nd4j-native:1.0.0-beta7:linux-x86_64\")\n",
      "}\n",
      "tasks.withType<ShadowJar> {\n",
      "\tbaseName = \"build-linux-windows\"\n",
      "\tsetZip64(true)\n",
      "\tdestinationDirectory.set(file(\"output\"))\n",
      "\tmergeServiceFiles()}\n",
      "//Add manifest - entry point\n",
      "tasks.withType(Jar::class) {\n",
      "    manifest {\n",
      "        attributes[\"Manifest-Version\"] = \"1.0\"\n",
      "        attributes[\"Main-Class\"] = \"ai.konduit.serving.cli.launcher.KonduitServingLauncher\"\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "Dependencies: [Dependency(ai.konduit.serving:konduit-serving-pipeline:0.1.0-SNAPSHOT), Dependency(ai.konduit.serving:konduit-serving-vertx:0.1.0-SNAPSHOT), Dependency(ai.konduit.serving:konduit-serving-cli:0.1.0-SNAPSHOT), Dependency(ai.konduit.serving:konduit-serving-http:0.1.0-SNAPSHOT), Dependency(ai.konduit.serving:konduit-serving-grpc:0.1.0-SNAPSHOT), Dependency(ai.konduit.serving:konduit-serving-deeplearning4j:0.1.0-SNAPSHOT), Dependency(org.nd4j:nd4j-native:1.0.0-beta7), Dependency(org.nd4j:nd4j-native:1.0.0-beta7:linux-x86_64)]\n",
      "Deployments: [UberJarDeployment(outputDir=output, jarName=build-linux-windows.jar, groupId=null, artifactId=null, version=null)]\n",
      ">> Build file generation complete\n",
      "\n",
      "\n",
      "Starting build...\n",
      "> Task :wrapper\n",
      "> Task :compileJava NO-SOURCE\n",
      "> Task :processResources NO-SOURCE\n",
      "> Task :classes UP-TO-DATE\n",
      "> Task :shadowJar\n",
      "\n",
      "Deprecated Gradle features were used in this build, making it incompatible with Gradle 7.0.\n",
      "Use '--warning-mode all' to show the individual deprecation warnings.\n",
      "See https://docs.gradle.org/6.4/userguide/command_line_interface.html#sec:command_line_warnings\n",
      "\n",
      "BUILD SUCCESSFUL in 4m 28s\n",
      "2 actionable tasks: 2 executed\n",
      "\n",
      ">> Build complete\n",
      "\n",
      "\n",
      "---------------------------------------- Build Summary -----------------------------------------\n",
      "Build duration:               268 sec\n",
      "Output artifacts:             1\n",
      "\n",
      " ----- UberJarDeployment -----\n",
      "JAR location:        /root/konduit/build/output/build-linux-windows.jar\n",
      "JAR size:            <JAR not found>\n",
      "JAR launch command:  java -jar <jar file name> <serve|list|stop|inspect|logs>\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "konduit build --arch x86 --modules deeplearning4j --serverType http,grpc --config jar.outputdir=output,jar.name=build-linux-windows.jar --os linux,windows --deploymentType JAR --pipeline config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upcoming support for Aurora Chips\n",
    "A new upcoming feature for Aurora chips is going to be included into konduit-serving binaries soon and the binaries can be created with using a command similar to the following:\n",
    "\n",
    "```bash\n",
    "konduit build --arch aurora --modules python,deeplearning4j --serverType http,grpc --config jar.outputdir=output,jar.name=build-aurora.jar --os linux --deploymentType JAR --pipeline config.json\n",
    "```\n",
    "\n",
    "Using the above build we'll be able to create a konduit-serving server for deeplearning4j that can be run on Aurora architecture chips and can be deployed on similar chips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Konduit-Serving KEY CONCEPTS\n",
    "\n",
    "## Pipeline \n",
    "In other words, a machine learning deployment (which contains models and optional code). The main logic of a pipeline is to have output of one step being sent as an input to the next step. Finally, output of the last step will be returned as an output to the user.\n",
    "A typical pipeline may look like: \n",
    "- preprocess input -> pass it to the model -> output\n",
    "```json\n",
    "{ \n",
    "  \"steps\" : [ {\n",
    "      \"@type\" : \"IMAGE_TO_NDARRAY\",\n",
    "      \"config\" : {\n",
    "        \"height\" : 28,\n",
    "        \"width\" : 28,\n",
    "        \"dataType\" : \"FLOAT\",\n",
    "        \"includeMinibatchDim\" : true,\n",
    "        \"aspectRatioHandling\" : \"CENTER_CROP\",\n",
    "        \"format\" : \"CHANNELS_FIRST\",\n",
    "        \"channelLayout\" : \"GRAYSCALE\",\n",
    "        \"normalization\" : {\n",
    "          \"type\" : \"SCALE\"\n",
    "        },\n",
    "        \"listHandling\" : \"NONE\"\n",
    "      },\n",
    "      \"keys\" : [ \"image\" ],\n",
    "      \"outputNames\" : [ \"Input3\" ],\n",
    "      \"keepOtherValues\" : true,\n",
    "      \"metadata\" : false,\n",
    "      \"metadataKey\" : \"@ImageToNDArrayStepMetadata\"\n",
    "    }, {\n",
    "      \"@type\" : \"ONNX\",\n",
    "      \"modelUri\" : \"mnist-8.onnx\",\n",
    "      \"inputNames\" : [ \"Input3\" ],\n",
    "      \"outputNames\" : [ \"Plus214_Output_0\" ]\n",
    "    } ] \n",
    "}\n",
    "```\n",
    "## Pipeline Step\n",
    "A pipeline step is a single component of a whole machine learning pipeline. Configurations are done through individual pipeline steps.\n",
    "```json\n",
    "{\n",
    "  \"@type\" : \"ONNX\",\n",
    "  \"modelUri\" : \"mnist-8.onnx\",\n",
    "  \"inputNames\" : [ \"Input3\" ],\n",
    "  \"outputNames\" : [ \"Plus214_Output_0\" ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Configuration\n",
    "JSON/YAML formatted data which defines a pipeline which could be model plus code and also contains server configuration as to what type of server to use and which host and post to use to run the server on.\n",
    "\n",
    "### Example\n",
    "Following is an example of a multi-step machine learning pipeline through konduit-serving:\n",
    "```json\n",
    "{\n",
    "  \"customEndpoints\": [ \"ai.konduit.OCREndPoints\" ],\n",
    "  \"host\" : \"localhost\",\n",
    "  \"port\" : 0,\n",
    "  \"protocol\" : \"HTTP\",\n",
    "  \"pipeline\" : {\n",
    "    \"steps\" : [ {\n",
    "      \"@type\" : \"IMAGE_TO_NDARRAY\",\n",
    "      \"config\" : {\n",
    "        \"height\" : 28,\n",
    "        \"width\" : 28,\n",
    "        \"dataType\" : \"FLOAT\",\n",
    "        \"includeMinibatchDim\" : true,\n",
    "        \"aspectRatioHandling\" : \"CENTER_CROP\",\n",
    "        \"format\" : \"CHANNELS_FIRST\",\n",
    "        \"channelLayout\" : \"GRAYSCALE\",\n",
    "        \"normalization\" : {\n",
    "          \"type\" : \"SCALE\"\n",
    "        },\n",
    "        \"listHandling\" : \"NONE\"\n",
    "      },\n",
    "      \"keys\" : [ \"image\" ],\n",
    "      \"outputNames\" : [ \"Input3\" ],\n",
    "      \"keepOtherValues\" : true,\n",
    "      \"metadata\" : false,\n",
    "      \"metadataKey\" : \"@ImageToNDArrayStepMetadata\"\n",
    "    }, {\n",
    "      \"@type\" : \"LOGGING\",\n",
    "      \"logLevel\" : \"INFO\",\n",
    "      \"log\" : \"KEYS_AND_VALUES\"\n",
    "    }, {\n",
    "      \"@type\" : \"ONNX\",\n",
    "      \"modelUri\" : \"mnist-8.onnx\",\n",
    "      \"inputNames\" : [ \"Input3\" ],\n",
    "      \"outputNames\" : [ \"Plus214_Output_0\" ]\n",
    "    } ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "The above configuration converts an image file sent to the server into a 28x28 n-dimensional array and logs the output to the console and then passes it to an ONNX model for final output.\n",
    "\n",
    "## Supported model types\n",
    " \n",
    "### ONNX\n",
    "Open Neural Network Exchange is a multi-framework/platform interchange format for trained deep learning models.\n",
    " \n",
    "#### Models Supported Via Onnx\n",
    "- Pytorch\n",
    "- MXNet \n",
    "- Others which can be converted into ONNX)\n",
    "\n",
    "### PMML (Experimental) \n",
    "Predictive Model Markup Language also a platform interchange format for mostly all of the traditional ML models such as random forest.\n",
    "\n",
    "#### Models supported via PMML \n",
    "- Scikit-learn \n",
    "- SparkML\n",
    "- XGBoost \n",
    "- Others with supports PMML conversion\n",
    "\n",
    "### Custom models\n",
    "Through custom Python or Java code\n",
    "\n",
    "### Other step types\n",
    "- Logging (for logging the data that's passing through a pipeline)\n",
    "```json\n",
    "{\n",
    "  \"@type\" : \"LOGGING\",\n",
    "  \"logLevel\" : \"INFO\",\n",
    "  \"log\" : \"KEYS_AND_VALUES\"\n",
    "}\n",
    "```\n",
    "- Image_to_ndarray (for converting image data to desired n-dimensional array)\n",
    "```json\n",
    " {\n",
    "  \"@type\" : \"IMAGE_TO_NDARRAY\",\n",
    "  \"config\" : {\n",
    "    \"height\" : 100,\n",
    "    \"width\" : 100,\n",
    "    \"dataType\" : \"FLOAT\",\n",
    "    \"includeMinibatchDim\" : true,\n",
    "    \"aspectRatioHandling\" : \"CENTER_CROP\",\n",
    "    \"format\" : \"CHANNELS_FIRST\",\n",
    "    \"channelLayout\" : \"RGB\",\n",
    "    \"normalization\" : {\n",
    "      \"type\" : \"SCALE\"\n",
    "    },\n",
    "    \"listHandling\" : \"NONE\"\n",
    "  },\n",
    "  \"keys\" : [ \"key1\", \"key2\" ],\n",
    "  \"outputNames\" : [ \"output1\", \"output2\" ],\n",
    "  \"keepOtherValues\" : true,\n",
    "  \"metadata\" : false,\n",
    "  \"metadataKey\" : \"@ImageToNDArrayStepMetadata\"\n",
    "}\n",
    "```\n",
    "- Camera (for taking input from a webcam)\n",
    "- ShowImage (for showing image output)\n",
    "- Video (for taking input from a video file)\n",
    "- Others \n",
    "\n",
    "# How konduit-serving will be used?\n",
    "- For users who want to host their machine learning model and code to production\n",
    "- Packaging and distribution platform for apps (marketplace)\n",
    "- Software + management for end-to-end products/solutions (verticals)\n",
    "\n",
    "## Example applications:\n",
    "- IoT/Edge\n",
    "    - Smart home, smart factory, smart city etc\n",
    "    - Telecommunication, facilities management\n",
    "- On-prem (\"on-premises\" - running locally on a company's server)\n",
    "    - Important for some industries - banking, healthcare etc (no private data in cloud)\n",
    "- Cloud hosting of models (konduit-serving on cloud)\n",
    "\n",
    "# KONDUIT SERVING CLI\n",
    "A user friendly command line interface to interact with konduit-serving. Convenient to be used for launching deployments, predicting outputs, inspecting servers and stopping them.\n",
    "\n",
    "The most used commands are:\n",
    "- config (for creating boilerplate configurations)\n",
    "```bash\n",
    "- Saves 'dl4j -> logging' config in a 'config.json' file:\n",
    "$ konduit config -p dl4j,logging -o config.json\n",
    "```\n",
    "- serve (for creating and running the server)\n",
    "```bash\n",
    "- Starts a server in the foreground with an id of 'inf_server' using\n",
    "'config.json' as configuration file:\n",
    "$ konduit serve -id inf_server -c config.json\n",
    "```\n",
    "- stop (for stopping the server)\n",
    "```bash\n",
    "- Stops the server with an id of 'inf_server':\n",
    "$ konduit stop inf_server\n",
    "```\n",
    "- list (lists the current running servers)\n",
    "```bash\n",
    "- Lists all the running servers\n",
    "$ konduit list\n",
    "```\n",
    "- logs (shows logs for a particular server)\n",
    "```bash\n",
    "- Outputs and tail the log file contents of server with an id of 'inf_server'\n",
    "  from the last 10 lines:\n",
    "$ konduit logs inf_server -l 10 -f\n",
    "```\n",
    "- inspect (for inspecting configuration of a particular server such as host port or number of steps)\n",
    "```bash\n",
    "- Queries the inference configuration of server with an id of 'inf_server'\n",
    "  based on the given pattern and gives output similar to\n",
    "  'localhost:45223-{<pipeline_details>}'. The curly brackets can be escaped.\n",
    "$ konduit inspect inf_server -q {host}:{port}-\\{{pipeline}\\}\n",
    "```\n",
    "- pythonpaths (for finding out the installed python binaries in the current server)\n",
    "```bash\n",
    "- Lists all the installed and registered CONDA python binaries:\n",
    "$ konduit pythonpaths list --type CONDA\n",
    "```\n",
    "\n",
    "# Support for custom REST Endpoints\n",
    "Custom rest endpoints can be defined through Java code (which can be easily created through jupyter). The following parameters should be defined for a custom endpoint definition:\n",
    "- consumes (such as application/json or multipart/form-data)\n",
    "- produces (same as consumes but for the REST output)\n",
    "- endpoint-name (such as /output)\n",
    "- logic-code (could be anything that processes the data in a specific format)\n",
    "\n",
    "## Example\n",
    "Following is a useful example for a custom endpoint code\n",
    "```java\n",
    "package ai.konduit;\n",
    "\n",
    "import ai.konduit.serving.endpoint.Endpoint;\n",
    "import ai.konduit.serving.pipeline.api.data.Data;\n",
    "import ai.konduit.serving.pipeline.api.data.Image;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.Pipeline;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.PipelineExecutor;\n",
    "import io.vertx.core.Handler;\n",
    "import io.vertx.core.http.HttpMethod;\n",
    "import io.vertx.ext.web.RoutingContext;\n",
    "\n",
    "import javax.imageio.ImageIO;\n",
    "import java.awt.image.BufferedImage;\n",
    "import java.io.File;\n",
    "import java.io.IOException;\n",
    "import java.util.ArrayList;\n",
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "import org.nd4j.linalg.factory.Nd4j;\n",
    "import ai.konduit.serving.pipeline.api.data.NDArray;\n",
    "import ai.konduit.serving.pipeline.util.ObjectMappers;\n",
    "\n",
    "import ai.konduit.serving.pipeline.registry.NDArrayConverterRegistry;\n",
    "import ai.konduit.serving.data.nd4j.format.ND4JConverters;\n",
    "    \n",
    "public class IrisEndPoint implements Endpoint {\n",
    "\n",
    "    private PipelineExecutor pipelineExecutor;\n",
    "\n",
    "    public IrisEndPoint(PipelineExecutor pipelineExecutor) { \n",
    "        this.pipelineExecutor = pipelineExecutor; \n",
    "        NDArrayConverterRegistry.addConverter(new ND4JConverters.Nd4jToSerializedConverter()); \n",
    "        NDArrayConverterRegistry.addConverter(new ND4JConverters.SerializedToNd4jArrConverter()); }\n",
    "\n",
    "    public HttpMethod type() { return HttpMethod.POST; }\n",
    "\n",
    "    public String path() { return \"/infer\"; }\n",
    "\n",
    "    public List<String> consumes() { return Arrays.asList(\"application/json\"); }\n",
    "\n",
    "    public List<String> produces() { return Arrays.asList(\"application/json\"); }\n",
    "\n",
    "    @Override\n",
    "    public Handler<RoutingContext> handler() {\n",
    "        return handler -> {\n",
    "            handler.vertx().executeBlocking(taskHandler -> {\n",
    "                Data data = Data.empty();\n",
    "                \n",
    "                try {\n",
    "                    data.put(\"input\", NDArray.create(Nd4j.create(new float[] {\n",
    "                        handler.getBodyAsJson().getFloat(\"sepal_length\"),\n",
    "                        handler.getBodyAsJson().getFloat(\"sepal_width\"),\n",
    "                        handler.getBodyAsJson().getFloat(\"petal_length\"),\n",
    "                        handler.getBodyAsJson().getFloat(\"petal_width\")}, new int[] { 1, 4 })));\n",
    "                } catch (Exception e) {\n",
    "                    e.printStackTrace();\n",
    "                }\n",
    "\n",
    "                Data exec = pipelineExecutor.exec(data);\n",
    "                \n",
    "                handler.response().end(ObjectMappers.toJson(exec.getNDArray(\"output\").getAs(float[].class)));\n",
    "                taskHandler.complete();\n",
    "            },resultHandler -> {\n",
    "                if(resultHandler.failed()) {\n",
    "                    if(resultHandler.cause() != null)\n",
    "                        if(handler.vertx().exceptionHandler() != null)\n",
    "                            handler.vertx().exceptionHandler().handle(resultHandler.cause());\n",
    "                        else {\n",
    "                            resultHandler.cause().printStackTrace();\n",
    "                        }\n",
    "                    else {\n",
    "                        System.err.println(\"Failed to process classification endpoint async task. Unknown cause.\");\n",
    "                    }\n",
    "                }\n",
    "            });\n",
    "\n",
    "        };\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "and (just a wrapper for the list of created endpoints)\n",
    "\n",
    "```java\n",
    "package ai.konduit;\n",
    "\n",
    "import ai.konduit.serving.endpoint.Endpoint;\n",
    "import ai.konduit.serving.endpoint.HttpEndpoints;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.Pipeline;\n",
    "import ai.konduit.serving.pipeline.api.pipeline.PipelineExecutor;\n",
    "\n",
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "public class IrisEndPoints implements HttpEndpoints {\n",
    "\n",
    "    @Override\n",
    "    public List<Endpoint> endpoints(Pipeline pipeline, PipelineExecutor pipelineExecutor) {\n",
    "        return Arrays.asList(new IrisEndPoint(pipelineExecutor));\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "This can be set inside the configuration through the key of `custom_endpoints` as: `custom_endpoints: [ \"ai.konduit.IrisEndPoints\" ]` \n",
    "For resolving the new code, the compiled code can be sent as an extra classpaths to the konduit-serving server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "text/x-java",
   "file_extension": ".java",
   "mimetype": "",
   "name": "Java",
   "nbconverter_exporter": "",
   "version": "1.8.0_121"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
